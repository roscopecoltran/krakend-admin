;
; Arachnado config file
;

[arachnado]
; General Arachnado server options.

; Event loop to use. Allowed values are
; "twisted", "tornado" and "auto".
reactor = auto

; Host/port to listen to
host = 0.0.0.0
port = 8888

; default log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
loglevel = INFO

; enable extra debug features
debug = 1


[arachnado.scrapy]
; Extra options passed to Scrapy by default.
; All upper-cased values are used as default Scrapy settings.
BOT_NAME = arachnado
DEPTH_LIMIT = 10

; path to a folder where to store disk request queues
; DISK_QUEUES_ROOT = ./.scrapy/jobs

; Packages to load spiders from (separated by whitespace)
spider_packages =

; Name of the default spider. It is used for crawling if
; no custom spider is specified or detected. It should support
; API similar to arachnado.spider.CrawlWebsiteSpider
; (which is the default here).
default_spider_name = generic

[arachnado.storage]
; Where to store crawled items and job information.
; Currently only MongoDB is supported (mongodb:// URIs).
enabled = 1

items_uri = mongodb://localhost:27017/arachnado/items
items_uri_env = ITEMS_MONGO_URI

jobs_uri = mongodb://localhost:27017/arachnado/jobs
jobs_uri_env = JOBS_MONGO_URI

sites_uri = mongodb://localhost:27017/arachnado/sites
sites_uri_env = SITES_MONGO_URI

[arachnado.manhole]
; Manhole options
enabled = 1
host = localhost
port = 6023

