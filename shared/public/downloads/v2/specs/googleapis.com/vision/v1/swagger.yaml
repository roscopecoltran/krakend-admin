swagger: '2.0'
schemes:
  - https
host: vision.googleapis.com
basePath: /
info:
  contact:
    name: Google
    url: 'https://google.com'
  description: 'Integrates Google Vision features, including image labeling, face, logo, and landmark detection, optical character recognition (OCR), and detection of explicit content, into applications.'
  title: Google Cloud Vision
  version: v1
  x-apiClientRegistration:
    url: 'https://console.developers.google.com'
  x-apisguru-categories:
    - machine_learning
    - analytics
  x-logo:
    url: 'https://api.apis.guru/v2/cache/logo/https_www.google.com_images_branding_googlelogo_2x_googlelogo_color_272x92dp.png'
  x-origin:
    - converter:
        url: 'https://github.com/lucybot/api-spec-converter'
        version: 2.6.0
      format: google
      url: 'https://vision.googleapis.com/$discovery/rest?version=v1'
      version: v1
  x-preferred: true
  x-providerName: googleapis.com
  x-serviceName: vision
externalDocs:
  url: 'https://cloud.google.com/vision/'
securityDefinitions:
  Oauth2:
    authorizationUrl: 'https://accounts.google.com/o/oauth2/auth'
    description: Oauth 2.0 authentication
    flow: implicit
    scopes:
      'https://www.googleapis.com/auth/cloud-platform': View and manage your data across Google Cloud Platform services
      'https://www.googleapis.com/auth/cloud-vision': Apply machine learning models to understand and label images
    type: oauth2
parameters:
  $.xgafv:
    description: V1 error format.
    enum:
      - '1'
      - '2'
    in: query
    name: $.xgafv
    type: string
  access_token:
    description: OAuth access token.
    in: query
    name: access_token
    type: string
  alt:
    default: json
    description: Data format for response.
    enum:
      - json
      - media
      - proto
    in: query
    name: alt
    type: string
  bearer_token:
    description: OAuth bearer token.
    in: query
    name: bearer_token
    type: string
  callback:
    description: JSONP
    in: query
    name: callback
    type: string
  fields:
    description: Selector specifying which fields to include in a partial response.
    in: query
    name: fields
    type: string
  key:
    description: 'API key. Your API key identifies your project and provides you with API access, quota, and reports. Required unless you provide an OAuth 2.0 token.'
    in: query
    name: key
    type: string
  oauth_token:
    description: OAuth 2.0 token for the current user.
    in: query
    name: oauth_token
    type: string
  pp:
    default: true
    description: Pretty-print response.
    in: query
    name: pp
    type: boolean
  prettyPrint:
    default: true
    description: Returns response with indentations and line breaks.
    in: query
    name: prettyPrint
    type: boolean
  quotaUser:
    description: 'Available to use for quota purposes for server-side applications. Can be any arbitrary string assigned to a user, but should not exceed 40 characters.'
    in: query
    name: quotaUser
    type: string
  uploadType:
    description: 'Legacy upload protocol for media (e.g. "media", "multipart").'
    in: query
    name: uploadType
    type: string
  upload_protocol:
    description: 'Upload protocol for media (e.g. "raw", "multipart").'
    in: query
    name: upload_protocol
    type: string
tags:
  - name: images
paths:
  '/v1/images:annotate':
    parameters:
      - $ref: '#/parameters/upload_protocol'
      - $ref: '#/parameters/prettyPrint'
      - $ref: '#/parameters/uploadType'
      - $ref: '#/parameters/fields'
      - $ref: '#/parameters/callback'
      - $ref: '#/parameters/$.xgafv'
      - $ref: '#/parameters/alt'
      - $ref: '#/parameters/key'
      - $ref: '#/parameters/access_token'
      - $ref: '#/parameters/quotaUser'
      - $ref: '#/parameters/pp'
      - $ref: '#/parameters/bearer_token'
      - $ref: '#/parameters/oauth_token'
    post:
      description: Run image detection and annotation for a batch of images.
      operationId: vision.images.annotate
      parameters:
        - in: body
          name: body
          schema:
            $ref: '#/definitions/BatchAnnotateImagesRequest'
      responses:
        '200':
          description: Successful response
          schema:
            $ref: '#/definitions/BatchAnnotateImagesResponse'
      security:
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-platform'
        - Oauth2:
            - 'https://www.googleapis.com/auth/cloud-vision'
      tags:
        - images
definitions:
  AnnotateImageRequest:
    description: |-
      Request for performing Google Cloud Vision API tasks over a user-provided
      image, with user-requested features.
    properties:
      features:
        description: Requested features.
        items:
          $ref: '#/definitions/Feature'
        type: array
      image:
        $ref: '#/definitions/Image'
        description: The image to be processed.
      imageContext:
        $ref: '#/definitions/ImageContext'
        description: Additional context that may accompany the image.
    type: object
  AnnotateImageResponse:
    description: Response to an image annotation request.
    properties:
      cropHintsAnnotation:
        $ref: '#/definitions/CropHintsAnnotation'
        description: 'If present, crop hints have completed successfully.'
      error:
        $ref: '#/definitions/Status'
        description: |-
          If set, represents the error message for the operation.
          Note that filled-in image annotations are guaranteed to be
          correct, even when `error` is set.
      faceAnnotations:
        description: 'If present, face detection has completed successfully.'
        items:
          $ref: '#/definitions/FaceAnnotation'
        type: array
      fullTextAnnotation:
        $ref: '#/definitions/TextAnnotation'
        description: |-
          If present, text (OCR) detection or document (OCR) text detection has
          completed successfully.
          This annotation provides the structural hierarchy for the OCR detected
          text.
      imagePropertiesAnnotation:
        $ref: '#/definitions/ImageProperties'
        description: 'If present, image properties were extracted successfully.'
      labelAnnotations:
        description: 'If present, label detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      landmarkAnnotations:
        description: 'If present, landmark detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      logoAnnotations:
        description: 'If present, logo detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      safeSearchAnnotation:
        $ref: '#/definitions/SafeSearchAnnotation'
        description: 'If present, safe-search annotation has completed successfully.'
      textAnnotations:
        description: 'If present, text (OCR) detection has completed successfully.'
        items:
          $ref: '#/definitions/EntityAnnotation'
        type: array
      webDetection:
        $ref: '#/definitions/WebDetection'
        description: 'If present, web detection has completed successfully.'
    type: object
  BatchAnnotateImagesRequest:
    description: Multiple image annotation requests are batched into a single service call.
    properties:
      requests:
        description: Individual image annotation requests for this batch.
        items:
          $ref: '#/definitions/AnnotateImageRequest'
        type: array
    type: object
  BatchAnnotateImagesResponse:
    description: Response to a batch image annotation request.
    properties:
      responses:
        description: Individual responses to image annotation requests within the batch.
        items:
          $ref: '#/definitions/AnnotateImageResponse'
        type: array
    type: object
  Block:
    description: Logical element on the page.
    properties:
      blockType:
        description: 'Detected block type (text, image etc) for this block.'
        enum:
          - UNKNOWN
          - TEXT
          - TABLE
          - PICTURE
          - RULER
          - BARCODE
        type: string
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the block.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      paragraphs:
        description: List of paragraphs in this block (if this blocks is of type text).
        items:
          $ref: '#/definitions/Paragraph'
        type: array
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the block.
    type: object
  BoundingPoly:
    description: A bounding polygon for the detected image annotation.
    properties:
      vertices:
        description: The bounding polygon vertices.
        items:
          $ref: '#/definitions/Vertex'
        type: array
    type: object
  Color:
    description: |-
      Represents a color in the RGBA color space. This representation is designed
      for simplicity of conversion to/from color representations in various
      languages over compactness; for example, the fields of this representation
      can be trivially provided to the constructor of "java.awt.Color" in Java; it
      can also be trivially provided to UIColor's "+colorWithRed:green:blue:alpha"
      method in iOS; and, with just a little work, it can be easily formatted into
      a CSS "rgba()" string in JavaScript, as well. Here are some examples:

      Example (Java):

           import com.google.type.Color;

           // ...
           public static java.awt.Color fromProto(Color protocolor) {
             float alpha = protocolor.hasAlpha()
                 ? protocolor.getAlpha().getValue()
                 : 1.0;

             return new java.awt.Color(
                 protocolor.getRed(),
                 protocolor.getGreen(),
                 protocolor.getBlue(),
                 alpha);
           }

           public static Color toProto(java.awt.Color color) {
             float red = (float) color.getRed();
             float green = (float) color.getGreen();
             float blue = (float) color.getBlue();
             float denominator = 255.0;
             Color.Builder resultBuilder =
                 Color
                     .newBuilder()
                     .setRed(red / denominator)
                     .setGreen(green / denominator)
                     .setBlue(blue / denominator);
             int alpha = color.getAlpha();
             if (alpha != 255) {
               result.setAlpha(
                   FloatValue
                       .newBuilder()
                       .setValue(((float) alpha) / denominator)
                       .build());
             }
             return resultBuilder.build();
           }
           // ...

      Example (iOS / Obj-C):

           // ...
           static UIColor* fromProto(Color* protocolor) {
              float red = [protocolor red];
              float green = [protocolor green];
              float blue = [protocolor blue];
              FloatValue* alpha_wrapper = [protocolor alpha];
              float alpha = 1.0;
              if (alpha_wrapper != nil) {
                alpha = [alpha_wrapper value];
              }
              return [UIColor colorWithRed:red green:green blue:blue alpha:alpha];
           }

           static Color* toProto(UIColor* color) {
               CGFloat red, green, blue, alpha;
               if (![color getRed:&red green:&green blue:&blue alpha:&alpha]) {
                 return nil;
               }
               Color* result = [Color alloc] init];
               [result setRed:red];
               [result setGreen:green];
               [result setBlue:blue];
               if (alpha <= 0.9999) {
                 [result setAlpha:floatWrapperWithValue(alpha)];
               }
               [result autorelease];
               return result;
          }
          // ...

       Example (JavaScript):

          // ...

          var protoToCssColor = function(rgb_color) {
             var redFrac = rgb_color.red || 0.0;
             var greenFrac = rgb_color.green || 0.0;
             var blueFrac = rgb_color.blue || 0.0;
             var red = Math.floor(redFrac * 255);
             var green = Math.floor(greenFrac * 255);
             var blue = Math.floor(blueFrac * 255);

             if (!('alpha' in rgb_color)) {
                return rgbToCssColor_(red, green, blue);
             }

             var alphaFrac = rgb_color.alpha.value || 0.0;
             var rgbParams = [red, green, blue].join(',');
             return ['rgba(', rgbParams, ',', alphaFrac, ')'].join('');
          };

          var rgbToCssColor_ = function(red, green, blue) {
            var rgbNumber = new Number((red << 16) | (green << 8) | blue);
            var hexString = rgbNumber.toString(16);
            var missingZeros = 6 - hexString.length;
            var resultBuilder = ['#'];
            for (var i = 0; i < missingZeros; i++) {
               resultBuilder.push('0');
            }
            resultBuilder.push(hexString);
            return resultBuilder.join('');
          };

          // ...
    properties:
      alpha:
        description: |-
          The fraction of this color that should be applied to the pixel. That is,
          the final pixel color is defined by the equation:

            pixel color = alpha * (this color) + (1.0 - alpha) * (background color)

          This means that a value of 1.0 corresponds to a solid color, whereas
          a value of 0.0 corresponds to a completely transparent color. This
          uses a wrapper message rather than a simple float scalar so that it is
          possible to distinguish between a default value and the value being unset.
          If omitted, this color object is to be rendered as a solid color
          (as if the alpha value had been explicitly given with a value of 1.0).
        format: float
        type: number
      blue:
        description: 'The amount of blue in the color as a value in the interval [0, 1].'
        format: float
        type: number
      green:
        description: 'The amount of green in the color as a value in the interval [0, 1].'
        format: float
        type: number
      red:
        description: 'The amount of red in the color as a value in the interval [0, 1].'
        format: float
        type: number
    type: object
  ColorInfo:
    description: |-
      Color information consists of RGB channels, score, and the fraction of
      the image that the color occupies in the image.
    properties:
      color:
        $ref: '#/definitions/Color'
        description: RGB components of the color.
      pixelFraction:
        description: |-
          The fraction of pixels the color occupies in the image.
          Value in range [0, 1].
        format: float
        type: number
      score:
        description: 'Image-specific score for this color. Value in range [0, 1].'
        format: float
        type: number
    type: object
  CropHint:
    description: Single crop hint that is used to generate a new crop when serving an image.
    properties:
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding polygon for the crop region. The coordinates of the bounding
          box are in the original image's scale, as returned in `ImageParams`.
      confidence:
        description: 'Confidence of this being a salient region.  Range [0, 1].'
        format: float
        type: number
      importanceFraction:
        description: |-
          Fraction of importance of this salient region with respect to the original
          image.
        format: float
        type: number
    type: object
  CropHintsAnnotation:
    description: Set of crop hints that are used to generate new crops when serving images.
    properties:
      cropHints:
        description: Crop hint results.
        items:
          $ref: '#/definitions/CropHint'
        type: array
    type: object
  CropHintsParams:
    description: Parameters for crop hints annotation request.
    properties:
      aspectRatios:
        description: |-
          Aspect ratios in floats, representing the ratio of the width to the height
          of the image. For example, if the desired aspect ratio is 4/3, the
          corresponding float value should be 1.33333.  If not specified, the
          best possible crop is returned. The number of provided aspect ratios is
          limited to a maximum of 16; any aspect ratios provided after the 16th are
          ignored.
        items:
          format: float
          type: number
        type: array
    type: object
  DetectedBreak:
    description: Detected start or end of a structural component.
    properties:
      isPrefix:
        description: True if break prepends the element.
        type: boolean
      type:
        description: Detected break type.
        enum:
          - UNKNOWN
          - SPACE
          - SURE_SPACE
          - EOL_SURE_SPACE
          - HYPHEN
          - LINE_BREAK
        type: string
    type: object
  DetectedLanguage:
    description: Detected language for a structural component.
    properties:
      confidence:
        description: 'Confidence of detected language. Range [0, 1].'
        format: float
        type: number
      languageCode:
        description: |-
          The BCP-47 language code, such as "en-US" or "sr-Latn". For more
          information, see
          http://www.unicode.org/reports/tr35/#Unicode_locale_identifier.
        type: string
    type: object
  DominantColorsAnnotation:
    description: Set of dominant colors and their corresponding scores.
    properties:
      colors:
        description: RGB color values with their score and pixel fraction.
        items:
          $ref: '#/definitions/ColorInfo'
        type: array
    type: object
  EntityAnnotation:
    description: Set of detected entity features.
    properties:
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          Image region to which this entity belongs. Not produced
          for `LABEL_DETECTION` features.
      confidence:
        description: |-
          The accuracy of the entity detection in an image.
          For example, for an image in which the "Eiffel Tower" entity is detected,
          this field represents the confidence that there is a tower in the query
          image. Range [0, 1].
        format: float
        type: number
      description:
        description: 'Entity textual description, expressed in its `locale` language.'
        type: string
      locale:
        description: |-
          The language code for the locale in which the entity textual
          `description` is expressed.
        type: string
      locations:
        description: |-
          The location information for the detected entity. Multiple
          `LocationInfo` elements can be present because one location may
          indicate the location of the scene in the image, and another location
          may indicate the location of the place where the image was taken.
          Location information is usually present for landmarks.
        items:
          $ref: '#/definitions/LocationInfo'
        type: array
      mid:
        description: |-
          Opaque entity ID. Some IDs may be available in
          [Google Knowledge Graph Search API](https://developers.google.com/knowledge-graph/).
        type: string
      properties:
        description: |-
          Some entities may have optional user-supplied `Property` (name/value)
          fields, such a score or string that qualifies the entity.
        items:
          $ref: '#/definitions/Property'
        type: array
      score:
        description: 'Overall score of the result. Range [0, 1].'
        format: float
        type: number
      topicality:
        description: |-
          The relevancy of the ICA (Image Content Annotation) label to the
          image. For example, the relevancy of "tower" is likely higher to an image
          containing the detected "Eiffel Tower" than to an image containing a
          detected distant towering building, even though the confidence that
          there is a tower in each image may be the same. Range [0, 1].
        format: float
        type: number
    type: object
  FaceAnnotation:
    description: A face annotation object contains the results of face detection.
    properties:
      angerLikelihood:
        description: Anger likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      blurredLikelihood:
        description: Blurred likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      boundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding polygon around the face. The coordinates of the bounding box
          are in the original image's scale, as returned in `ImageParams`.
          The bounding box is computed to "frame" the face in accordance with human
          expectations. It is based on the landmarker results.
          Note that one or more x and/or y coordinates may not be generated in the
          `BoundingPoly` (the polygon will be unbounded) if only a partial face
          appears in the image to be annotated.
      detectionConfidence:
        description: 'Detection confidence. Range [0, 1].'
        format: float
        type: number
      fdBoundingPoly:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The `fd_bounding_poly` bounding polygon is tighter than the
          `boundingPoly`, and encloses only the skin part of the face. Typically, it
          is used to eliminate the face from any image analysis that detects the
          "amount of skin" visible in an image. It is not based on the
          landmarker results, only on the initial face detection, hence
          the <code>fd</code> (face detection) prefix.
      headwearLikelihood:
        description: Headwear likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      joyLikelihood:
        description: Joy likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      landmarkingConfidence:
        description: 'Face landmarking confidence. Range [0, 1].'
        format: float
        type: number
      landmarks:
        description: Detected face landmarks.
        items:
          $ref: '#/definitions/Landmark'
        type: array
      panAngle:
        description: |-
          Yaw angle, which indicates the leftward/rightward angle that the face is
          pointing relative to the vertical plane perpendicular to the image. Range
          [-180,180].
        format: float
        type: number
      rollAngle:
        description: |-
          Roll angle, which indicates the amount of clockwise/anti-clockwise rotation
          of the face relative to the image vertical about the axis perpendicular to
          the face. Range [-180,180].
        format: float
        type: number
      sorrowLikelihood:
        description: Sorrow likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      surpriseLikelihood:
        description: Surprise likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      tiltAngle:
        description: |-
          Pitch angle, which indicates the upwards/downwards angle that the face is
          pointing relative to the image's horizontal plane. Range [-180,180].
        format: float
        type: number
      underExposedLikelihood:
        description: Under-exposed likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  Feature:
    description: |-
      Users describe the type of Google Cloud Vision API tasks to perform over
      images by using *Feature*s. Each Feature indicates a type of image
      detection task to perform. Features encode the Cloud Vision API
      vertical to operate on and the number of top-scoring results to return.
    properties:
      maxResults:
        description: Maximum number of results of this type.
        format: int32
        type: integer
      type:
        description: The feature type.
        enum:
          - TYPE_UNSPECIFIED
          - FACE_DETECTION
          - LANDMARK_DETECTION
          - LOGO_DETECTION
          - LABEL_DETECTION
          - TEXT_DETECTION
          - DOCUMENT_TEXT_DETECTION
          - SAFE_SEARCH_DETECTION
          - IMAGE_PROPERTIES
          - CROP_HINTS
          - WEB_DETECTION
        type: string
    type: object
  Image:
    description: Client image to perform Google Cloud Vision API tasks over.
    properties:
      content:
        description: |-
          Image content, represented as a stream of bytes.
          Note: as with all `bytes` fields, protobuffers use a pure binary
          representation, whereas JSON representations use base64.
        format: byte
        type: string
      source:
        $ref: '#/definitions/ImageSource'
        description: |-
          Google Cloud Storage image location. If both `content` and `source`
          are provided for an image, `content` takes precedence and is
          used to perform the image annotation request.
    type: object
  ImageContext:
    description: Image context and/or feature-specific parameters.
    properties:
      cropHintsParams:
        $ref: '#/definitions/CropHintsParams'
        description: Parameters for crop hints annotation request.
      languageHints:
        description: |-
          List of languages to use for TEXT_DETECTION. In most cases, an empty value
          yields the best results since it enables automatic language detection. For
          languages based on the Latin alphabet, setting `language_hints` is not
          needed. In rare cases, when the language of the text in the image is known,
          setting a hint will help get better results (although it will be a
          significant hindrance if the hint is wrong). Text detection returns an
          error if one or more of the specified languages is not one of the
          [supported languages](/vision/docs/languages).
        items:
          type: string
        type: array
      latLongRect:
        $ref: '#/definitions/LatLongRect'
        description: lat/long rectangle that specifies the location of the image.
    type: object
  ImageProperties:
    description: 'Stores image properties, such as dominant colors.'
    properties:
      dominantColors:
        $ref: '#/definitions/DominantColorsAnnotation'
        description: 'If present, dominant colors completed successfully.'
    type: object
  ImageSource:
    description: External image source (Google Cloud Storage image location).
    properties:
      gcsImageUri:
        description: |-
          NOTE: For new code `image_uri` below is preferred.
          Google Cloud Storage image URI, which must be in the following form:
          `gs://bucket_name/object_name` (for details, see
          [Google Cloud Storage Request
          URIs](https://cloud.google.com/storage/docs/reference-uris)).
          NOTE: Cloud Storage object versioning is not supported.
        type: string
      imageUri:
        description: |-
          Image URI which supports:
          1) Google Cloud Storage image URI, which must be in the following form:
          `gs://bucket_name/object_name` (for details, see
          [Google Cloud Storage Request
          URIs](https://cloud.google.com/storage/docs/reference-uris)).
          NOTE: Cloud Storage object versioning is not supported.
          2) Publicly accessible image HTTP/HTTPS URL.
          This is preferred over the legacy `gcs_image_uri` above. When both
          `gcs_image_uri` and `image_uri` are specified, `image_uri` takes
          precedence.
        type: string
    type: object
  Landmark:
    description: |-
      A face-specific landmark (for example, a face feature).
      Landmark positions may fall outside the bounds of the image
      if the face is near one or more edges of the image.
      Therefore it is NOT guaranteed that `0 <= x < width` or
      `0 <= y < height`.
    properties:
      position:
        $ref: '#/definitions/Position'
        description: Face landmark position.
      type:
        description: Face landmark type.
        enum:
          - UNKNOWN_LANDMARK
          - LEFT_EYE
          - RIGHT_EYE
          - LEFT_OF_LEFT_EYEBROW
          - RIGHT_OF_LEFT_EYEBROW
          - LEFT_OF_RIGHT_EYEBROW
          - RIGHT_OF_RIGHT_EYEBROW
          - MIDPOINT_BETWEEN_EYES
          - NOSE_TIP
          - UPPER_LIP
          - LOWER_LIP
          - MOUTH_LEFT
          - MOUTH_RIGHT
          - MOUTH_CENTER
          - NOSE_BOTTOM_RIGHT
          - NOSE_BOTTOM_LEFT
          - NOSE_BOTTOM_CENTER
          - LEFT_EYE_TOP_BOUNDARY
          - LEFT_EYE_RIGHT_CORNER
          - LEFT_EYE_BOTTOM_BOUNDARY
          - LEFT_EYE_LEFT_CORNER
          - RIGHT_EYE_TOP_BOUNDARY
          - RIGHT_EYE_RIGHT_CORNER
          - RIGHT_EYE_BOTTOM_BOUNDARY
          - RIGHT_EYE_LEFT_CORNER
          - LEFT_EYEBROW_UPPER_MIDPOINT
          - RIGHT_EYEBROW_UPPER_MIDPOINT
          - LEFT_EAR_TRAGION
          - RIGHT_EAR_TRAGION
          - LEFT_EYE_PUPIL
          - RIGHT_EYE_PUPIL
          - FOREHEAD_GLABELLA
          - CHIN_GNATHION
          - CHIN_LEFT_GONION
          - CHIN_RIGHT_GONION
        type: string
    type: object
  LatLng:
    description: |-
      An object representing a latitude/longitude pair. This is expressed as a pair
      of doubles representing degrees latitude and degrees longitude. Unless
      specified otherwise, this must conform to the
      <a href="http://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf">WGS84
      standard</a>. Values must be within normalized ranges.

      Example of normalization code in Python:

          def NormalizeLongitude(longitude):
            """Wraps decimal degrees longitude to [-180.0, 180.0]."""
            q, r = divmod(longitude, 360.0)
            if r > 180.0 or (r == 180.0 and q <= -1.0):
              return r - 360.0
            return r

          def NormalizeLatLng(latitude, longitude):
            """Wraps decimal degrees latitude and longitude to
            [-90.0, 90.0] and [-180.0, 180.0], respectively."""
            r = latitude % 360.0
            if r <= 90.0:
              return r, NormalizeLongitude(longitude)
            elif r >= 270.0:
              return r - 360, NormalizeLongitude(longitude)
            else:
              return 180 - r, NormalizeLongitude(longitude + 180.0)

          assert 180.0 == NormalizeLongitude(180.0)
          assert -180.0 == NormalizeLongitude(-180.0)
          assert -179.0 == NormalizeLongitude(181.0)
          assert (0.0, 0.0) == NormalizeLatLng(360.0, 0.0)
          assert (0.0, 0.0) == NormalizeLatLng(-360.0, 0.0)
          assert (85.0, 180.0) == NormalizeLatLng(95.0, 0.0)
          assert (-85.0, -170.0) == NormalizeLatLng(-95.0, 10.0)
          assert (90.0, 10.0) == NormalizeLatLng(90.0, 10.0)
          assert (-90.0, -10.0) == NormalizeLatLng(-90.0, -10.0)
          assert (0.0, -170.0) == NormalizeLatLng(-180.0, 10.0)
          assert (0.0, -170.0) == NormalizeLatLng(180.0, 10.0)
          assert (-90.0, 10.0) == NormalizeLatLng(270.0, 10.0)
          assert (90.0, 10.0) == NormalizeLatLng(-270.0, 10.0)
    properties:
      latitude:
        description: 'The latitude in degrees. It must be in the range [-90.0, +90.0].'
        format: double
        type: number
      longitude:
        description: 'The longitude in degrees. It must be in the range [-180.0, +180.0].'
        format: double
        type: number
    type: object
  LatLongRect:
    description: Rectangle determined by min and max `LatLng` pairs.
    properties:
      maxLatLng:
        $ref: '#/definitions/LatLng'
        description: Max lat/long pair.
      minLatLng:
        $ref: '#/definitions/LatLng'
        description: Min lat/long pair.
    type: object
  LocationInfo:
    description: Detected entity location information.
    properties:
      latLng:
        $ref: '#/definitions/LatLng'
        description: lat/long location coordinates.
    type: object
  Page:
    description: Detected page from OCR.
    properties:
      blocks:
        description: 'List of blocks of text, images etc on this page.'
        items:
          $ref: '#/definitions/Block'
        type: array
      height:
        description: Page height in pixels.
        format: int32
        type: integer
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected on the page.
      width:
        description: Page width in pixels.
        format: int32
        type: integer
    type: object
  Paragraph:
    description: Structural unit of text representing a number of words in certain order.
    properties:
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the paragraph.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the paragraph.
      words:
        description: List of words in this paragraph.
        items:
          $ref: '#/definitions/Word'
        type: array
    type: object
  Position:
    description: |-
      A 3D position in the image, used primarily for Face detection landmarks.
      A valid Position must have both x and y coordinates.
      The position coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: float
        type: number
      'y':
        description: Y coordinate.
        format: float
        type: number
      z:
        description: Z coordinate (or depth).
        format: float
        type: number
    type: object
  Property:
    description: A `Property` consists of a user-supplied name/value pair.
    properties:
      name:
        description: Name of the property.
        type: string
      uint64Value:
        description: Value of numeric properties.
        format: uint64
        type: string
      value:
        description: Value of the property.
        type: string
    type: object
  SafeSearchAnnotation:
    description: |-
      Set of features pertaining to the image, computed by computer vision
      methods over safe-search verticals (for example, adult, spoof, medical,
      violence).
    properties:
      adult:
        description: Represents the adult content likelihood for the image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      medical:
        description: Likelihood that this is a medical image.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      spoof:
        description: |-
          Spoof likelihood. The likelihood that an modification
          was made to the image's canonical version to make it appear
          funny or offensive.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
      violence:
        description: Violence likelihood.
        enum:
          - UNKNOWN
          - VERY_UNLIKELY
          - UNLIKELY
          - POSSIBLE
          - LIKELY
          - VERY_LIKELY
        type: string
    type: object
  Status:
    description: |-
      The `Status` type defines a logical error model that is suitable for different
      programming environments, including REST APIs and RPC APIs. It is used by
      [gRPC](https://github.com/grpc). The error model is designed to be:

      - Simple to use and understand for most users
      - Flexible enough to meet unexpected needs

      # Overview

      The `Status` message contains three pieces of data: error code, error message,
      and error details. The error code should be an enum value of
      google.rpc.Code, but it may accept additional error codes if needed.  The
      error message should be a developer-facing English message that helps
      developers *understand* and *resolve* the error. If a localized user-facing
      error message is needed, put the localized message in the error details or
      localize it in the client. The optional error details may contain arbitrary
      information about the error. There is a predefined set of error detail types
      in the package `google.rpc` that can be used for common error conditions.

      # Language mapping

      The `Status` message is the logical representation of the error model, but it
      is not necessarily the actual wire format. When the `Status` message is
      exposed in different client libraries and different wire protocols, it can be
      mapped differently. For example, it will likely be mapped to some exceptions
      in Java, but more likely mapped to some error codes in C.

      # Other uses

      The error model and the `Status` message can be used in a variety of
      environments, either with or without APIs, to provide a
      consistent developer experience across different environments.

      Example uses of this error model include:

      - Partial errors. If a service needs to return partial errors to the client,
          it may embed the `Status` in the normal response to indicate the partial
          errors.

      - Workflow errors. A typical workflow has multiple steps. Each step may
          have a `Status` message for error reporting.

      - Batch operations. If a client uses batch request and batch response, the
          `Status` message should be used directly inside batch response, one for
          each error sub-response.

      - Asynchronous operations. If an API call embeds asynchronous operation
          results in its response, the status of those operations should be
          represented directly using the `Status` message.

      - Logging. If some API errors are stored in logs, the message `Status` could
          be used directly after any stripping needed for security/privacy reasons.
    properties:
      code:
        description: 'The status code, which should be an enum value of google.rpc.Code.'
        format: int32
        type: integer
      details:
        description: |-
          A list of messages that carry the error details.  There is a common set of
          message types for APIs to use.
        items:
          additionalProperties:
            description: Properties of the object. Contains field @type with type URL.
          type: object
        type: array
      message:
        description: |-
          A developer-facing error message, which should be in English. Any
          user-facing error message should be localized and sent in the
          google.rpc.Status.details field, or localized by the client.
        type: string
    type: object
  Symbol:
    description: A single symbol representation.
    properties:
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the symbol.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the symbol.
      text:
        description: The actual UTF-8 representation of the symbol.
        type: string
    type: object
  TextAnnotation:
    description: |-
      TextAnnotation contains a structured representation of OCR extracted text.
      The hierarchy of an OCR extracted text structure is like this:
          TextAnnotation -> Page -> Block -> Paragraph -> Word -> Symbol
      Each structural component, starting from Page, may further have their own
      properties. Properties describe detected languages, breaks etc.. Please
      refer to the google.cloud.vision.v1.TextAnnotation.TextProperty message
      definition below for more detail.
    properties:
      pages:
        description: List of pages detected by OCR.
        items:
          $ref: '#/definitions/Page'
        type: array
      text:
        description: UTF-8 text detected on the pages.
        type: string
    type: object
  TextProperty:
    description: Additional information detected on the structural component.
    properties:
      detectedBreak:
        $ref: '#/definitions/DetectedBreak'
        description: Detected start or end of a text segment.
      detectedLanguages:
        description: A list of detected languages together with confidence.
        items:
          $ref: '#/definitions/DetectedLanguage'
        type: array
    type: object
  Vertex:
    description: |-
      A vertex represents a 2D point in the image.
      NOTE: the vertex coordinates are in the same scale as the original image.
    properties:
      x:
        description: X coordinate.
        format: int32
        type: integer
      'y':
        description: Y coordinate.
        format: int32
        type: integer
    type: object
  WebDetection:
    description: Relevant information for the image from the Internet.
    properties:
      fullMatchingImages:
        description: |-
          Fully matching images from the Internet.
          Can include resized copies of the query image.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      pagesWithMatchingImages:
        description: Web pages containing the matching images from the Internet.
        items:
          $ref: '#/definitions/WebPage'
        type: array
      partialMatchingImages:
        description: |-
          Partial matching images from the Internet.
          Those images are similar enough to share some key-point features. For
          example an original image will likely have partial matching for its crops.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      visuallySimilarImages:
        description: The visually similar image results.
        items:
          $ref: '#/definitions/WebImage'
        type: array
      webEntities:
        description: Deduced entities from similar images on the Internet.
        items:
          $ref: '#/definitions/WebEntity'
        type: array
    type: object
  WebEntity:
    description: Entity deduced from similar images on the Internet.
    properties:
      description:
        description: 'Canonical description of the entity, in English.'
        type: string
      entityId:
        description: Opaque entity ID.
        type: string
      score:
        description: |-
          Overall relevancy score for the entity.
          Not normalized and not comparable across different image queries.
        format: float
        type: number
    type: object
  WebImage:
    description: Metadata for online images.
    properties:
      score:
        description: (Deprecated) Overall relevancy score for the image.
        format: float
        type: number
      url:
        description: The result image URL.
        type: string
    type: object
  WebPage:
    description: Metadata for web pages.
    properties:
      score:
        description: (Deprecated) Overall relevancy score for the web page.
        format: float
        type: number
      url:
        description: The result web page URL.
        type: string
    type: object
  Word:
    description: A word representation.
    properties:
      boundingBox:
        $ref: '#/definitions/BoundingPoly'
        description: |-
          The bounding box for the word.
          The vertices are in the order of top-left, top-right, bottom-right,
          bottom-left. When a rotation of the bounding box is detected the rotation
          is represented as around the top-left corner as defined when the text is
          read in the 'natural' orientation.
          For example:
            * when the text is horizontal it might look like:
               0----1
               |    |
               3----2
            * when it's rotated 180 degrees around the top-left corner it becomes:
               2----3
               |    |
               1----0
            and the vertice order will still be (0, 1, 2, 3).
      property:
        $ref: '#/definitions/TextProperty'
        description: Additional information detected for the word.
      symbols:
        description: |-
          List of symbols in the word.
          The order of the symbols follows the natural reading order.
        items:
          $ref: '#/definitions/Symbol'
        type: array
    type: object
