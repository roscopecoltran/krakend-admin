syntax = "proto3";

import "google/api/annotations.proto";

package googledataflow;

message PostV1b3ProjectsProjectIdWorkerMessagesRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    SendWorkerMessagesRequest body = 5;
    string callback = 6;
    string fields = 7;
    string key = 8;
    string oauth_token = 9;
    boolean pp = 10;
    boolean prettyPrint = 11;
    // The project to send the WorkerMessages to.
    string projectId = 12;
    string quotaUser = 13;
    string uploadType = 14;
    string upload_protocol = 15;
}

message GetV1b3ProjectsProjectIdJobsRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // The kind of filter to use.
    enum GetV1b3ProjectsProjectIdJobsRequest_Filter {
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_FILTER_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_FILTER_ALL = 1;
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_FILTER_TERMINATED = 2;
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_FILTER_ACTIVE = 3;
    }
    GetV1b3ProjectsProjectIdJobsRequest_Filter filter = 7;
    string key = 8;
    // The location that contains this job.
    string location = 9;
    string oauth_token = 10;
    // If there are many jobs, limit response to at most this many.
    // The actual number of jobs returned will be the lesser of max_responses
    // and an unspecified server-defined limit.
    int32 pageSize = 11;
    // Set this to the 'next_page_token' field of a previous response
    // to request additional results in a long list.
    string pageToken = 12;
    boolean pp = 13;
    boolean prettyPrint = 14;
    // The project which owns the jobs.
    string projectId = 15;
    string quotaUser = 16;
    string uploadType = 17;
    string upload_protocol = 18;
    // Level of information requested in response. Default is `JOB_VIEW_SUMMARY`.
    enum GetV1b3ProjectsProjectIdJobsRequest_View {
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_SUMMARY = 1;
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_ALL = 2;
        GETV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_DESCRIPTION = 3;
    }
    GetV1b3ProjectsProjectIdJobsRequest_View view = 19;
}

message PostV1b3ProjectsProjectIdJobsRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    Job body = 5;
    string callback = 6;
    string fields = 7;
    string key = 8;
    // The location that contains this job.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The ID of the Cloud Platform project that the job belongs to.
    string projectId = 13;
    string quotaUser = 14;
    // Deprecated. This field is now in the Job message.
    string replaceJobId = 15;
    string uploadType = 16;
    string upload_protocol = 17;
    // The level of information requested in response.
    enum PostV1b3ProjectsProjectIdJobsRequest_View {
        POSTV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_UNKNOWN = 0;
        POSTV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_SUMMARY = 1;
        POSTV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_ALL = 2;
        POSTV1B3PROJECTSPROJECTIDJOBSREQUEST_VIEW_JOB_VIEW_DESCRIPTION = 3;
    }
    PostV1b3ProjectsProjectIdJobsRequest_View view = 18;
}

message GetV1b3ProjectsProjectIdJobsJobIdRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // The job ID.
    string jobId = 7;
    string key = 8;
    // The location that contains this job.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The ID of the Cloud Platform project that the job belongs to.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
    // The level of information requested in response.
    enum GetV1b3ProjectsProjectIdJobsJobIdRequest_View {
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDREQUEST_VIEW_JOB_VIEW_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDREQUEST_VIEW_JOB_VIEW_SUMMARY = 1;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDREQUEST_VIEW_JOB_VIEW_ALL = 2;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDREQUEST_VIEW_JOB_VIEW_DESCRIPTION = 3;
    }
    GetV1b3ProjectsProjectIdJobsJobIdRequest_View view = 17;
}

message PutV1b3ProjectsProjectIdJobsJobIdRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    Job body = 5;
    string callback = 6;
    string fields = 7;
    // The job ID.
    string jobId = 8;
    string key = 9;
    // The location that contains this job.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // The ID of the Cloud Platform project that the job belongs to.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message PostV1b3ProjectsProjectIdJobsJobIdDebugGetConfigRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    GetDebugConfigRequest body = 5;
    string callback = 6;
    string fields = 7;
    // The job id.
    string jobId = 8;
    string key = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The project id.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
}

message PostV1b3ProjectsProjectIdJobsJobIdDebugSendCaptureRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    SendDebugCaptureRequest body = 5;
    string callback = 6;
    string fields = 7;
    // The job id.
    string jobId = 8;
    string key = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The project id.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
}

message GetV1b3ProjectsProjectIdJobsJobIdMessagesRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    // Return only messages with timestamps < end_time. The default is now
    // (i.e. return up to the latest messages available).
    string endTime = 6;
    string fields = 7;
    // The job to get messages about.
    string jobId = 8;
    string key = 9;
    // The location which contains the job specified by job_id.
    string location = 10;
    // Filter to only get messages with importance >= level
    enum GetV1b3ProjectsProjectIdJobsJobIdMessagesRequest_MinimumImportance {
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_IMPORTANCE_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_DEBUG = 1;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_DETAILED = 2;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_BASIC = 3;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_WARNING = 4;
        GETV1B3PROJECTSPROJECTIDJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_ERROR = 5;
    }
    GetV1b3ProjectsProjectIdJobsJobIdMessagesRequest_MinimumImportance minimumImportance = 11;
    string oauth_token = 12;
    // If specified, determines the maximum number of messages to
    // return.  If unspecified, the service may choose an appropriate
    // default, or may return an arbitrarily large number of results.
    int32 pageSize = 13;
    // If supplied, this should be the value of next_page_token returned
    // by an earlier call. This will cause the next page of results to
    // be returned.
    string pageToken = 14;
    boolean pp = 15;
    boolean prettyPrint = 16;
    // A project id.
    string projectId = 17;
    string quotaUser = 18;
    // If specified, return only messages with timestamps >= start_time.
    // The default is the job creation time (i.e. beginning of messages).
    string startTime = 19;
    string uploadType = 20;
    string upload_protocol = 21;
}

message GetV1b3ProjectsProjectIdJobsJobIdMetricsRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // The job to get messages for.
    string jobId = 7;
    string key = 8;
    // The location which contains the job specified by job_id.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // A project id.
    string projectId = 13;
    string quotaUser = 14;
    // Return only metric data that has changed since this time.
    // Default is to return all information about all metrics for the job.
    string startTime = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message PostV1b3ProjectsProjectIdJobsJobIdWorkItems:LeaseRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    LeaseWorkItemRequest body = 5;
    string callback = 6;
    string fields = 7;
    // Identifies the workflow job this worker belongs to.
    string jobId = 8;
    string key = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // Identifies the project this worker belongs to.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
}

message PostV1b3ProjectsProjectIdJobsJobIdWorkItems:ReportStatusRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    ReportWorkItemStatusRequest body = 5;
    string callback = 6;
    string fields = 7;
    // The job which the WorkItem is part of.
    string jobId = 8;
    string key = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The project which owns the WorkItem's job.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
}

message GetV1b3ProjectsProjectIdJobs:AggregatedRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // The kind of filter to use.
    enum GetV1b3ProjectsProjectIdJobs:AggregatedRequest_Filter {
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_FILTER_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_FILTER_ALL = 1;
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_FILTER_TERMINATED = 2;
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_FILTER_ACTIVE = 3;
    }
    GetV1b3ProjectsProjectIdJobs:AggregatedRequest_Filter filter = 7;
    string key = 8;
    // The location that contains this job.
    string location = 9;
    string oauth_token = 10;
    // If there are many jobs, limit response to at most this many.
    // The actual number of jobs returned will be the lesser of max_responses
    // and an unspecified server-defined limit.
    int32 pageSize = 11;
    // Set this to the 'next_page_token' field of a previous response
    // to request additional results in a long list.
    string pageToken = 12;
    boolean pp = 13;
    boolean prettyPrint = 14;
    // The project which owns the jobs.
    string projectId = 15;
    string quotaUser = 16;
    string uploadType = 17;
    string upload_protocol = 18;
    // Level of information requested in response. Default is `JOB_VIEW_SUMMARY`.
    enum GetV1b3ProjectsProjectIdJobs:AggregatedRequest_View {
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_VIEW_JOB_VIEW_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_VIEW_JOB_VIEW_SUMMARY = 1;
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_VIEW_JOB_VIEW_ALL = 2;
        GETV1B3PROJECTSPROJECTIDJOBS:AGGREGATEDREQUEST_VIEW_JOB_VIEW_DESCRIPTION = 3;
    }
    GetV1b3ProjectsProjectIdJobs:AggregatedRequest_View view = 19;
}

message PostV1b3ProjectsProjectIdLocationsLocationWorkerMessagesRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    SendWorkerMessagesRequest body = 5;
    string callback = 6;
    string fields = 7;
    string key = 8;
    // The location which contains the job
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The project to send the WorkerMessages to.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
}

message GetV1b3ProjectsProjectIdLocationsLocationJobsRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // The kind of filter to use.
    enum GetV1b3ProjectsProjectIdLocationsLocationJobsRequest_Filter {
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_FILTER_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_FILTER_ALL = 1;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_FILTER_TERMINATED = 2;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_FILTER_ACTIVE = 3;
    }
    GetV1b3ProjectsProjectIdLocationsLocationJobsRequest_Filter filter = 7;
    string key = 8;
    // The location that contains this job.
    string location = 9;
    string oauth_token = 10;
    // If there are many jobs, limit response to at most this many.
    // The actual number of jobs returned will be the lesser of max_responses
    // and an unspecified server-defined limit.
    int32 pageSize = 11;
    // Set this to the 'next_page_token' field of a previous response
    // to request additional results in a long list.
    string pageToken = 12;
    boolean pp = 13;
    boolean prettyPrint = 14;
    // The project which owns the jobs.
    string projectId = 15;
    string quotaUser = 16;
    string uploadType = 17;
    string upload_protocol = 18;
    // Level of information requested in response. Default is `JOB_VIEW_SUMMARY`.
    enum GetV1b3ProjectsProjectIdLocationsLocationJobsRequest_View {
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_SUMMARY = 1;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_ALL = 2;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_DESCRIPTION = 3;
    }
    GetV1b3ProjectsProjectIdLocationsLocationJobsRequest_View view = 19;
}

message PostV1b3ProjectsProjectIdLocationsLocationJobsRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    Job body = 5;
    string callback = 6;
    string fields = 7;
    string key = 8;
    // The location that contains this job.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The ID of the Cloud Platform project that the job belongs to.
    string projectId = 13;
    string quotaUser = 14;
    // Deprecated. This field is now in the Job message.
    string replaceJobId = 15;
    string uploadType = 16;
    string upload_protocol = 17;
    // The level of information requested in response.
    enum PostV1b3ProjectsProjectIdLocationsLocationJobsRequest_View {
        POSTV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_UNKNOWN = 0;
        POSTV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_SUMMARY = 1;
        POSTV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_ALL = 2;
        POSTV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSREQUEST_VIEW_JOB_VIEW_DESCRIPTION = 3;
    }
    PostV1b3ProjectsProjectIdLocationsLocationJobsRequest_View view = 18;
}

message GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // The job ID.
    string jobId = 7;
    string key = 8;
    // The location that contains this job.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // The ID of the Cloud Platform project that the job belongs to.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
    // The level of information requested in response.
    enum GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdRequest_View {
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDREQUEST_VIEW_JOB_VIEW_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDREQUEST_VIEW_JOB_VIEW_SUMMARY = 1;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDREQUEST_VIEW_JOB_VIEW_ALL = 2;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDREQUEST_VIEW_JOB_VIEW_DESCRIPTION = 3;
    }
    GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdRequest_View view = 17;
}

message PutV1b3ProjectsProjectIdLocationsLocationJobsJobIdRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    Job body = 5;
    string callback = 6;
    string fields = 7;
    // The job ID.
    string jobId = 8;
    string key = 9;
    // The location that contains this job.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // The ID of the Cloud Platform project that the job belongs to.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdDebugGetConfigRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    GetDebugConfigRequest body = 5;
    string callback = 6;
    string fields = 7;
    // The job id.
    string jobId = 8;
    string key = 9;
    // The location which contains the job specified by job_id.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // The project id.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdDebugSendCaptureRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    SendDebugCaptureRequest body = 5;
    string callback = 6;
    string fields = 7;
    // The job id.
    string jobId = 8;
    string key = 9;
    // The location which contains the job specified by job_id.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // The project id.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMessagesRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    // Return only messages with timestamps < end_time. The default is now
    // (i.e. return up to the latest messages available).
    string endTime = 6;
    string fields = 7;
    // The job to get messages about.
    string jobId = 8;
    string key = 9;
    // The location which contains the job specified by job_id.
    string location = 10;
    // Filter to only get messages with importance >= level
    enum GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMessagesRequest_MinimumImportance {
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_IMPORTANCE_UNKNOWN = 0;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_DEBUG = 1;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_DETAILED = 2;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_BASIC = 3;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_WARNING = 4;
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONJOBSJOBIDMESSAGESREQUEST_MINIMUMIMPORTANCE_JOB_MESSAGE_ERROR = 5;
    }
    GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMessagesRequest_MinimumImportance minimumImportance = 11;
    string oauth_token = 12;
    // If specified, determines the maximum number of messages to
    // return.  If unspecified, the service may choose an appropriate
    // default, or may return an arbitrarily large number of results.
    int32 pageSize = 13;
    // If supplied, this should be the value of next_page_token returned
    // by an earlier call. This will cause the next page of results to
    // be returned.
    string pageToken = 14;
    boolean pp = 15;
    boolean prettyPrint = 16;
    // A project id.
    string projectId = 17;
    string quotaUser = 18;
    // If specified, return only messages with timestamps >= start_time.
    // The default is the job creation time (i.e. beginning of messages).
    string startTime = 19;
    string uploadType = 20;
    string upload_protocol = 21;
}

message GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMetricsRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // The job to get messages for.
    string jobId = 7;
    string key = 8;
    // The location which contains the job specified by job_id.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // A project id.
    string projectId = 13;
    string quotaUser = 14;
    // Return only metric data that has changed since this time.
    // Default is to return all information about all metrics for the job.
    string startTime = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdWorkItems:LeaseRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    LeaseWorkItemRequest body = 5;
    string callback = 6;
    string fields = 7;
    // Identifies the workflow job this worker belongs to.
    string jobId = 8;
    string key = 9;
    // The location which contains the WorkItem's job.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // Identifies the project this worker belongs to.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdWorkItems:ReportStatusRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    ReportWorkItemStatusRequest body = 5;
    string callback = 6;
    string fields = 7;
    // The job which the WorkItem is part of.
    string jobId = 8;
    string key = 9;
    // The location which contains the WorkItem's job.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // The project which owns the WorkItem's job.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
}

message PostV1b3ProjectsProjectIdLocationsLocationTemplatesRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    CreateJobFromTemplateRequest body = 5;
    string callback = 6;
    string fields = 7;
    string key = 8;
    // The location to which to direct the request.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // Required. The ID of the Cloud Platform project that the job belongs to.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
}

message GetV1b3ProjectsProjectIdLocationsLocationTemplates:GetRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // Required. A Cloud Storage path to the template from which to
    // create the job.
    // Must be a valid Cloud Storage URL, beginning with `gs://`.
    string gcsPath = 7;
    string key = 8;
    // The location to which to direct the request.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // Required. The ID of the Cloud Platform project that the job belongs to.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
    // The view to retrieve. Defaults to METADATA_ONLY.
    enum GetV1b3ProjectsProjectIdLocationsLocationTemplates:GetRequest_View {
        GETV1B3PROJECTSPROJECTIDLOCATIONSLOCATIONTEMPLATES:GETREQUEST_VIEW_METADATA_ONLY = 0;
    }
    GetV1b3ProjectsProjectIdLocationsLocationTemplates:GetRequest_View view = 17;
}

message PostV1b3ProjectsProjectIdLocationsLocationTemplates:LaunchRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    LaunchTemplateParameters body = 5;
    string callback = 6;
    string fields = 7;
    // Required. A Cloud Storage path to the template from which to create
    // the job.
    // Must be valid Cloud Storage URL, beginning with 'gs://'.
    string gcsPath = 8;
    string key = 9;
    // The location to which to direct the request.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // Required. The ID of the Cloud Platform project that the job belongs to.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
    // If true, the request is validated but not actually executed.
    // Defaults to false.
    bool validateOnly = 18;
}

message PostV1b3ProjectsProjectIdTemplatesRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    CreateJobFromTemplateRequest body = 5;
    string callback = 6;
    string fields = 7;
    string key = 8;
    string oauth_token = 9;
    boolean pp = 10;
    boolean prettyPrint = 11;
    // Required. The ID of the Cloud Platform project that the job belongs to.
    string projectId = 12;
    string quotaUser = 13;
    string uploadType = 14;
    string upload_protocol = 15;
}

message GetV1b3ProjectsProjectIdTemplates:GetRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    string callback = 5;
    string fields = 6;
    // Required. A Cloud Storage path to the template from which to
    // create the job.
    // Must be a valid Cloud Storage URL, beginning with `gs://`.
    string gcsPath = 7;
    string key = 8;
    // The location to which to direct the request.
    string location = 9;
    string oauth_token = 10;
    boolean pp = 11;
    boolean prettyPrint = 12;
    // Required. The ID of the Cloud Platform project that the job belongs to.
    string projectId = 13;
    string quotaUser = 14;
    string uploadType = 15;
    string upload_protocol = 16;
    // The view to retrieve. Defaults to METADATA_ONLY.
    enum GetV1b3ProjectsProjectIdTemplates:GetRequest_View {
        GETV1B3PROJECTSPROJECTIDTEMPLATES:GETREQUEST_VIEW_METADATA_ONLY = 0;
    }
    GetV1b3ProjectsProjectIdTemplates:GetRequest_View view = 17;
}

message PostV1b3ProjectsProjectIdTemplates:LaunchRequest {
    $.xgafv $.xgafv = 1;
    string access_token = 2;
    alt alt = 3;
    string bearer_token = 4;
    LaunchTemplateParameters body = 5;
    string callback = 6;
    string fields = 7;
    // Required. A Cloud Storage path to the template from which to create
    // the job.
    // Must be valid Cloud Storage URL, beginning with 'gs://'.
    string gcsPath = 8;
    string key = 9;
    // The location to which to direct the request.
    string location = 10;
    string oauth_token = 11;
    boolean pp = 12;
    boolean prettyPrint = 13;
    // Required. The ID of the Cloud Platform project that the job belongs to.
    string projectId = 14;
    string quotaUser = 15;
    string uploadType = 16;
    string upload_protocol = 17;
    // If true, the request is validated but not actually executed.
    // Defaults to false.
    bool validateOnly = 18;
}

enum $.Xgafv {
    $XGAFV_1 = 0;
    $XGAFV_2 = 1;
}

message ApproximateProgress {
    // Obsolete.
    float percentComplete = 1;
    // Obsolete.
    Position position = 2;
    // Obsolete.
    string remainingTime = 3;
}

message ApproximateReportedProgress {
    // Total amount of parallelism in the portion of input of this task that has
    // already been consumed and is no longer active. In the first two examples
    // above (see remaining_parallelism), the value should be 29 or 2
    // respectively.  The sum of remaining_parallelism and consumed_parallelism
    // should equal the total amount of parallelism in this work item.  If
    // specified, must be finite.
    ReportedParallelism consumedParallelism = 1;
    // Completion as fraction of the input consumed, from 0.0 (beginning, nothing
    // consumed), to 1.0 (end of the input, entire input consumed).
    double fractionConsumed = 2;
    // A Position within the work to represent a progress.
    Position position = 3;
    // Total amount of parallelism in the input of this task that remains,
    // (i.e. can be delegated to this task and any new tasks via dynamic
    // splitting). Always at least 1 for non-finished work items and 0 for
    // finished.
    // 
    // "Amount of parallelism" refers to how many non-empty parts of the input
    // can be read in parallel. This does not necessarily equal number
    // of records. An input that can be read in parallel down to the
    // individual records is called "perfectly splittable".
    // An example of non-perfectly parallelizable input is a block-compressed
    // file format where a block of records has to be read as a whole,
    // but different blocks can be read in parallel.
    // 
    // Examples:
    // * If we are processing record #30 (starting at 1) out of 50 in a perfectly
    //   splittable 50-record input, this value should be 21 (20 remaining + 1
    //   current).
    // * If we are reading through block 3 in a block-compressed file consisting
    //   of 5 blocks, this value should be 3 (since blocks 4 and 5 can be
    //   processed in parallel by new tasks via dynamic splitting and the current
    //   task remains processing block 3).
    // * If we are reading through the last block in a block-compressed file,
    //   or reading or processing the last record in a perfectly splittable
    //   input, this value should be 1, because apart from the current task, no
    //   additional remainder can be split off.
    ReportedParallelism remainingParallelism = 4;
}

message ApproximateSplitRequest {
    // A fraction at which to split the work item, from 0.0 (beginning of the
    // input) to 1.0 (end of the input).
    double fractionConsumed = 1;
    // A Position at which to split the work item.
    Position position = 2;
}

message AutoscalingEvent {
    // The current number of workers the job has.
    string currentNumWorkers = 1;
    // A message describing why the system decided to adjust the current
    // number of workers, why it failed, or why the system decided to
    // not make any changes to the number of workers.
    StructuredMessage description = 2;
    // The type of autoscaling event to report.
    enum AutoscalingEvent_EventType {
        AUTOSCALINGEVENT_EVENTTYPE_TYPE_UNKNOWN = 0;
        AUTOSCALINGEVENT_EVENTTYPE_TARGET_NUM_WORKERS_CHANGED = 1;
        AUTOSCALINGEVENT_EVENTTYPE_CURRENT_NUM_WORKERS_CHANGED = 2;
        AUTOSCALINGEVENT_EVENTTYPE_ACTUATION_FAILURE = 3;
        AUTOSCALINGEVENT_EVENTTYPE_NO_CHANGE = 4;
    }
    AutoscalingEvent_EventType eventType = 3;
    // The target number of workers the worker pool wants to resize to use.
    string targetNumWorkers = 4;
    // The time this event was emitted to indicate a new target or current
    // num_workers value.
    string time = 5;
}

message AutoscalingSettings {
    // The algorithm to use for autoscaling.
    enum AutoscalingSettings_Algorithm {
        AUTOSCALINGSETTINGS_ALGORITHM_AUTOSCALING_ALGORITHM_UNKNOWN = 0;
        AUTOSCALINGSETTINGS_ALGORITHM_AUTOSCALING_ALGORITHM_NONE = 1;
        AUTOSCALINGSETTINGS_ALGORITHM_AUTOSCALING_ALGORITHM_BASIC = 2;
    }
    AutoscalingSettings_Algorithm algorithm = 1;
    // The maximum number of workers to cap scaling at.
    int32 maxNumWorkers = 2;
}

message CPUTime {
    // Average CPU utilization rate (% non-idle cpu / second) since previous
    // sample.
    double rate = 1;
    // Timestamp of the measurement.
    string timestamp = 2;
    // Total active CPU time across all cores (ie., non-idle) in milliseconds
    // since start-up.
    string totalMs = 3;
}

message ComponentSource {
    // Dataflow service generated name for this source.
    string name = 1;
    // User name for the original user transform or collection with which this
    // source is most closely associated.
    string originalTransformOrCollection = 2;
    // Human-readable name for this transform; may be user or system generated.
    string userName = 3;
}

message ComponentTransform {
    // Dataflow service generated name for this source.
    string name = 1;
    // User name for the original user transform with which this transform is
    // most closely associated.
    string originalTransform = 2;
    // Human-readable name for this transform; may be user or system generated.
    string userName = 3;
}

message ComputationTopology {
    // The ID of the computation.
    string computationId = 1;
    // The inputs to the computation.
    repeated StreamLocation inputs = 2;
    // The key ranges processed by the computation.
    repeated KeyRangeLocation keyRanges = 3;
    // The outputs from the computation.
    repeated StreamLocation outputs = 4;
    // The state family values.
    repeated StateFamilyConfig stateFamilies = 5;
    // The system stage name.
    string systemStageName = 6;
}

message ConcatPosition {
    // Index of the inner source.
    int32 index = 1;
    // Position within the inner source.
    Position position = 2;
}

message CounterMetadata {
    // Human-readable description of the counter semantics.
    string description = 1;
    // Counter aggregation kind.
    enum CounterMetadata_Kind {
        COUNTERMETADATA_KIND_INVALID = 0;
        COUNTERMETADATA_KIND_SUM = 1;
        COUNTERMETADATA_KIND_MAX = 2;
        COUNTERMETADATA_KIND_MIN = 3;
        COUNTERMETADATA_KIND_MEAN = 4;
        COUNTERMETADATA_KIND_OR = 5;
        COUNTERMETADATA_KIND_AND = 6;
        COUNTERMETADATA_KIND_SET = 7;
        COUNTERMETADATA_KIND_DISTRIBUTION = 8;
    }
    CounterMetadata_Kind kind = 2;
    // A string referring to the unit type.
    string otherUnits = 3;
    // System defined Units, see above enum.
    enum CounterMetadata_StandardUnit {
        COUNTERMETADATA_STANDARDUNIT_BYTES = 0;
        COUNTERMETADATA_STANDARDUNIT_BYTES_PER_SEC = 1;
        COUNTERMETADATA_STANDARDUNIT_MILLISECONDS = 2;
        COUNTERMETADATA_STANDARDUNIT_MICROSECONDS = 3;
        COUNTERMETADATA_STANDARDUNIT_NANOSECONDS = 4;
        COUNTERMETADATA_STANDARDUNIT_TIMESTAMP_MSEC = 5;
        COUNTERMETADATA_STANDARDUNIT_TIMESTAMP_USEC = 6;
        COUNTERMETADATA_STANDARDUNIT_TIMESTAMP_NSEC = 7;
    }
    CounterMetadata_StandardUnit standardUnits = 4;
}

message CounterStructuredName {
    // Name of the optimized step being executed by the workers.
    string componentStepName = 1;
    // Name of the stage. An execution step contains multiple component steps.
    string executionStepName = 2;
    // Counter name. Not necessarily globally-unique, but unique within the
    // context of the other fields.
    // Required.
    string name = 3;
    // One of the standard Origins defined above.
    enum CounterStructuredName_Origin {
        COUNTERSTRUCTUREDNAME_ORIGIN_SYSTEM = 0;
        COUNTERSTRUCTUREDNAME_ORIGIN_USER = 1;
    }
    CounterStructuredName_Origin origin = 4;
    // A string containing a more specific namespace of the counter's origin.
    string originNamespace = 5;
    // The GroupByKey step name from the original graph.
    string originalShuffleStepName = 6;
    // System generated name of the original step in the user's graph, before
    // optimization.
    string originalStepName = 7;
    // Portion of this counter, either key or value.
    enum CounterStructuredName_Portion {
        COUNTERSTRUCTUREDNAME_PORTION_ALL = 0;
        COUNTERSTRUCTUREDNAME_PORTION_KEY = 1;
        COUNTERSTRUCTUREDNAME_PORTION_VALUE = 2;
    }
    CounterStructuredName_Portion portion = 8;
    // ID of a side input being read from/written to. Side inputs are identified
    // by a pair of (reader, input_index). The reader is usually equal to the
    // original name, but it may be different, if a ParDo emits it's Iterator /
    // Map side input object.
    SideInputId sideInput = 9;
    // ID of a particular worker.
    string workerId = 10;
}

message CounterStructuredNameAndMetadata {
    // Metadata associated with a counter
    CounterMetadata metadata = 1;
    // Structured name of the counter.
    CounterStructuredName name = 2;
}

message CounterUpdate {
    // Boolean value for And, Or.
    bool boolean = 1;
    // True if this counter is reported as the total cumulative aggregate
    // value accumulated since the worker started working on this WorkItem.
    // By default this is false, indicating that this counter is reported
    // as a delta.
    bool cumulative = 2;
    // Distribution data
    DistributionUpdate distribution = 3;
    // Floating point value for Sum, Max, Min.
    double floatingPoint = 4;
    // List of floating point numbers, for Set.
    FloatingPointList floatingPointList = 5;
    // Floating point mean aggregation value for Mean.
    FloatingPointMean floatingPointMean = 6;
    // Integer value for Sum, Max, Min.
    SplitInt64 integer = 7;
    // List of integers, for Set.
    IntegerList integerList = 8;
    // Integer mean aggregation value for Mean.
    IntegerMean integerMean = 9;
    // Value for internally-defined counters used by the Dataflow service.
    ;
    // Counter name and aggregation type.
    NameAndKind nameAndKind = 11;
    // The service-generated short identifier for this counter.
    // The short_id -> (name, metadata) mapping is constant for the lifetime of
    // a job.
    string shortId = 12;
    // List of strings, for Set.
    StringList stringList = 13;
    // Counter structured name and metadata.
    CounterStructuredNameAndMetadata structuredNameAndMetadata = 14;
}

message CreateJobFromTemplateRequest {
    // The runtime environment for the job.
    RuntimeEnvironment environment = 1;
    // Required. A Cloud Storage path to the template from which to
    // create the job.
    // Must be a valid Cloud Storage URL, beginning with `gs://`.
    string gcsPath = 2;
    // Required. The job name to use for the created job.
    string jobName = 3;
    // The location to which to direct the request.
    string location = 4;
    // The runtime parameters to pass to the job.
    map<string, string> parameters = 5;
}

message CustomSourceLocation {
    // Whether this source is stateful.
    bool stateful = 1;
}

message DataDiskAssignment {
    // Mounted data disks. The order is important a data disk's 0-based index in
    // this list defines which persistent directory the disk is mounted to, for
    // example the list of { "myproject-1014-104817-4c2-harness-0-disk-0" },
    // { "myproject-1014-104817-4c2-harness-0-disk-1" }.
    repeated string dataDisks = 1;
    // VM instance name the data disks mounted to, for example
    // "myproject-1014-104817-4c2-harness-0".
    string vmInstance = 2;
}

message DerivedSource {
    // What source to base the produced source on (if any).
    enum DerivedSource_DerivationMode {
        DERIVEDSOURCE_DERIVATIONMODE_SOURCE_DERIVATION_MODE_UNKNOWN = 0;
        DERIVEDSOURCE_DERIVATIONMODE_SOURCE_DERIVATION_MODE_INDEPENDENT = 1;
        DERIVEDSOURCE_DERIVATIONMODE_SOURCE_DERIVATION_MODE_CHILD_OF_CURRENT = 2;
        DERIVEDSOURCE_DERIVATIONMODE_SOURCE_DERIVATION_MODE_SIBLING_OF_CURRENT = 3;
    }
    DerivedSource_DerivationMode derivationMode = 1;
    // Specification of the source.
    Source source = 2;
}

message Disk {
    // Disk storage type, as defined by Google Compute Engine.  This
    // must be a disk type appropriate to the project and zone in which
    // the workers will run.  If unknown or unspecified, the service
    // will attempt to choose a reasonable default.
    // 
    // For example, the standard persistent disk type is a resource name
    // typically ending in "pd-standard".  If SSD persistent disks are
    // available, the resource name typically ends with "pd-ssd".  The
    // actual valid values are defined the Google Compute Engine API,
    // not by the Cloud Dataflow API; consult the Google Compute Engine
    // documentation for more information about determining the set of
    // available disk types for a particular project and zone.
    // 
    // Google Compute Engine Disk types are local to a particular
    // project in a particular zone, and so the resource name will
    // typically look something like this:
    // 
    // compute.googleapis.com/projects/project-id/zones/zone/diskTypes/pd-standard
    string diskType = 1;
    // Directory in a VM where disk is mounted.
    string mountPoint = 2;
    // Size of disk in GB.  If zero or unspecified, the service will
    // attempt to choose a reasonable default.
    int32 sizeGb = 3;
}

message DisplayData {
    // Contains value if the data is of a boolean type.
    bool boolValue = 1;
    // Contains value if the data is of duration type.
    string durationValue = 2;
    // Contains value if the data is of float type.
    float floatValue = 3;
    // Contains value if the data is of int64 type.
    string int64Value = 4;
    // Contains value if the data is of java class type.
    string javaClassValue = 5;
    // The key identifying the display data.
    // This is intended to be used as a label for the display data
    // when viewed in a dax monitoring system.
    string key = 6;
    // An optional label to display in a dax UI for the element.
    string label = 7;
    // The namespace for the key. This is usually a class name or programming
    // language namespace (i.e. python module) which defines the display data.
    // This allows a dax monitoring system to specially handle the data
    // and perform custom rendering.
    string namespace = 8;
    // A possible additional shorter value to display.
    // For example a java_class_name_value of com.mypackage.MyDoFn
    // will be stored with MyDoFn as the short_str_value and
    // com.mypackage.MyDoFn as the java_class_name value.
    // short_str_value can be displayed and java_class_name_value
    // will be displayed as a tooltip.
    string shortStrValue = 9;
    // Contains value if the data is of string type.
    string strValue = 10;
    // Contains value if the data is of timestamp type.
    string timestampValue = 11;
    // An optional full URL.
    string url = 12;
}

message DistributionUpdate {
    // The count of the number of elements present in the distribution.
    SplitInt64 count = 1;
    // (Optional) Histogram of value counts for the distribution.
    Histogram histogram = 2;
    // The maximum value present in the distribution.
    SplitInt64 max = 3;
    // The minimum value present in the distribution.
    SplitInt64 min = 4;
    // Use an int64 since we'd prefer the added precision. If overflow is a common
    // problem we can detect it and use an additional int64 or a double.
    SplitInt64 sum = 5;
    // Use a double since the sum of squares is likely to overflow int64.
    double sumOfSquares = 6;
}

message DynamicSourceSplit {
    // Primary part (continued to be processed by worker).
    // Specified relative to the previously-current source.
    // Becomes current.
    DerivedSource primary = 1;
    // Residual part (returned to the pool of work).
    // Specified relative to the previously-current source.
    DerivedSource residual = 2;
}

message Environment {
    // The type of cluster manager API to use.  If unknown or
    // unspecified, the service will attempt to choose a reasonable
    // default.  This should be in the form of the API service name,
    // e.g. "compute.googleapis.com".
    string clusterManagerApiService = 1;
    // The dataset for the current project where various workflow
    // related tables are stored.
    // 
    // The supported resource type is:
    // 
    // Google BigQuery:
    //   bigquery.googleapis.com/{dataset}
    string dataset = 2;
    // The list of experiments to enable.
    repeated string experiments = 3;
    // Experimental settings.
    map<string, > internalExperiments = 4;
    // The Cloud Dataflow SDK pipeline options specified by the user. These
    // options are passed through the service and are used to recreate the
    // SDK pipeline options on the worker in a language agnostic and platform
    // independent way.
    map<string, > sdkPipelineOptions = 5;
    // Identity to run virtual machines as. Defaults to the default account.
    string serviceAccountEmail = 6;
    // The prefix of the resources the system should use for temporary
    // storage.  The system will append the suffix "/temp-{JOBNAME} to
    // this resource prefix, where {JOBNAME} is the value of the
    // job_name field.  The resulting bucket and object prefix is used
    // as the prefix of the resources used to store temporary data
    // needed during the job execution.  NOTE: This will override the
    // value in taskrunner_settings.
    // The supported resource type is:
    // 
    // Google Cloud Storage:
    // 
    //   storage.googleapis.com/{bucket}/{object}
    //   bucket.storage.googleapis.com/{object}
    string tempStoragePrefix = 7;
    // A description of the process that generated the request.
    map<string, > userAgent = 8;
    // A structure describing which components and their versions of the service
    // are required in order to run the job.
    map<string, > version = 9;
    // The worker pools. At least one "harness" worker pool must be
    // specified in order for the job to have workers.
    repeated WorkerPool workerPools = 10;
}

message ExecutionStageState {
    // The time at which the stage transitioned to this state.
    string currentStateTime = 1;
    // The name of the execution stage.
    string executionStageName = 2;
    // Executions stage states allow the same set of values as JobState.
    enum ExecutionStageState_ExecutionStageState {
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_UNKNOWN = 0;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_STOPPED = 1;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_RUNNING = 2;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_DONE = 3;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_FAILED = 4;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_CANCELLED = 5;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_UPDATED = 6;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_DRAINING = 7;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_DRAINED = 8;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_PENDING = 9;
        EXECUTIONSTAGESTATE_EXECUTIONSTAGESTATE_JOB_STATE_CANCELLING = 10;
    }
    ExecutionStageState_ExecutionStageState executionStageState = 3;
}

message ExecutionStageSummary {
    // Collections produced and consumed by component transforms of this stage.
    repeated ComponentSource componentSource = 1;
    // Transforms that comprise this execution stage.
    repeated ComponentTransform componentTransform = 2;
    // Dataflow service generated id for this stage.
    string id = 3;
    // Input sources for this stage.
    repeated StageSource inputSource = 4;
    // Type of tranform this stage is executing.
    enum ExecutionStageSummary_Kind {
        EXECUTIONSTAGESUMMARY_KIND_UNKNOWN_KIND = 0;
        EXECUTIONSTAGESUMMARY_KIND_PAR_DO_KIND = 1;
        EXECUTIONSTAGESUMMARY_KIND_GROUP_BY_KEY_KIND = 2;
        EXECUTIONSTAGESUMMARY_KIND_FLATTEN_KIND = 3;
        EXECUTIONSTAGESUMMARY_KIND_READ_KIND = 4;
        EXECUTIONSTAGESUMMARY_KIND_WRITE_KIND = 5;
        EXECUTIONSTAGESUMMARY_KIND_CONSTANT_KIND = 6;
        EXECUTIONSTAGESUMMARY_KIND_SINGLETON_KIND = 7;
        EXECUTIONSTAGESUMMARY_KIND_SHUFFLE_KIND = 8;
    }
    ExecutionStageSummary_Kind kind = 5;
    // Dataflow service generated name for this stage.
    string name = 6;
    // Output sources for this stage.
    repeated StageSource outputSource = 7;
}

message FailedLocation {
    // The name of the failed location.
    string name = 1;
}

message FlattenInstruction {
    // Describes the inputs to the flatten instruction.
    repeated InstructionInput inputs = 1;
}

message FloatingPointList {
    // Elements of the list.
    repeated double elements = 1;
}

message FloatingPointMean {
    // The number of values being aggregated.
    SplitInt64 count = 1;
    // The sum of all values being aggregated.
    double sum = 2;
}

message GetDebugConfigRequest {
    // The internal component id for which debug configuration is
    // requested.
    string componentId = 1;
    // The location which contains the job specified by job_id.
    string location = 2;
    // The worker id, i.e., VM hostname.
    string workerId = 3;
}

message GetDebugConfigResponse {
    // The encoded debug configuration for the requested component.
    string config = 1;
}

message GetTemplateResponse {
    // The template metadata describing the template name, available
    // parameters, etc.
    TemplateMetadata metadata = 1;
    // The status of the get template request. Any problems with the
    // request will be indicated in the error_details.
    Status status = 2;
}

message Histogram {
    // Counts of values in each bucket. For efficiency, prefix and trailing
    // buckets with count = 0 are elided. Buckets can store the full range of
    // values of an unsigned long, with ULLONG_MAX falling into the 59th bucket
    // with range [1e19, 2e19).
    repeated string bucketCounts = 1;
    // Starting index of first stored bucket. The non-inclusive upper-bound of
    // the ith bucket is given by:
    //   pow(10,(i-first_bucket_offset)/3) * (1,2,5)[(i-first_bucket_offset)%3]
    int32 firstBucketOffset = 2;
}

message InstructionInput {
    // The output index (origin zero) within the producer.
    int32 outputNum = 1;
    // The index (origin zero) of the parallel instruction that produces
    // the output to be consumed by this input.  This index is relative
    // to the list of instructions in this input's instruction's
    // containing MapTask.
    int32 producerInstructionIndex = 2;
}

message InstructionOutput {
    // The codec to use to encode data being written via this output.
    map<string, > codec = 1;
    // The user-provided name of this output.
    string name = 2;
    // For system-generated byte and mean byte metrics, certain instructions
    // should only report the key size.
    bool onlyCountKeyBytes = 3;
    // For system-generated byte and mean byte metrics, certain instructions
    // should only report the value size.
    bool onlyCountValueBytes = 4;
    // System-defined name for this output in the original workflow graph.
    // Outputs that do not contribute to an original instruction do not set this.
    string originalName = 5;
    // System-defined name of this output.
    // Unique across the workflow.
    string systemName = 6;
}

message IntegerList {
    // Elements of the list.
    repeated SplitInt64 elements = 1;
}

message IntegerMean {
    // The number of values being aggregated.
    SplitInt64 count = 1;
    // The sum of all values being aggregated.
    SplitInt64 sum = 2;
}

message Job {
    // The client's unique identifier of the job, re-used across retried attempts.
    // If this field is set, the service will ensure its uniqueness.
    // The request to create a job will fail if the service has knowledge of a
    // previously submitted job with the same client's ID and job name.
    // The caller may use this field to ensure idempotence of job
    // creation across retried attempts to create a job.
    // By default, the field is empty and, in that case, the service ignores it.
    string clientRequestId = 1;
    // The timestamp when the job was initially created. Immutable and set by the
    // Cloud Dataflow service.
    string createTime = 2;
    // The current state of the job.
    // 
    // Jobs are created in the `JOB_STATE_STOPPED` state unless otherwise
    // specified.
    // 
    // A job in the `JOB_STATE_RUNNING` state may asynchronously enter a
    // terminal state. After a job has reached a terminal state, no
    // further state updates may be made.
    // 
    // This field may be mutated by the Cloud Dataflow service;
    // callers cannot mutate it.
    enum Job_CurrentState {
        JOB_CURRENTSTATE_JOB_STATE_UNKNOWN = 0;
        JOB_CURRENTSTATE_JOB_STATE_STOPPED = 1;
        JOB_CURRENTSTATE_JOB_STATE_RUNNING = 2;
        JOB_CURRENTSTATE_JOB_STATE_DONE = 3;
        JOB_CURRENTSTATE_JOB_STATE_FAILED = 4;
        JOB_CURRENTSTATE_JOB_STATE_CANCELLED = 5;
        JOB_CURRENTSTATE_JOB_STATE_UPDATED = 6;
        JOB_CURRENTSTATE_JOB_STATE_DRAINING = 7;
        JOB_CURRENTSTATE_JOB_STATE_DRAINED = 8;
        JOB_CURRENTSTATE_JOB_STATE_PENDING = 9;
        JOB_CURRENTSTATE_JOB_STATE_CANCELLING = 10;
    }
    Job_CurrentState currentState = 3;
    // The timestamp associated with the current state.
    string currentStateTime = 4;
    // The environment for the job.
    Environment environment = 5;
    // Deprecated.
    JobExecutionInfo executionInfo = 6;
    // The unique ID of this job.
    // 
    // This field is set by the Cloud Dataflow service when the Job is
    // created, and is immutable for the life of the job.
    string id = 7;
    // User-defined labels for this job.
    // 
    // The labels map can contain no more than 64 entries.  Entries of the labels
    // map are UTF8 strings that comply with the following restrictions:
    // 
    // * Keys must conform to regexp:  \p{Ll}\p{Lo}{0,62}
    // * Values must conform to regexp:  [\p{Ll}\p{Lo}\p{N}_-]{0,63}
    // * Both keys and values are additionally constrained to be <= 128 bytes in
    // size.
    map<string, string> labels = 8;
    // The location that contains this job.
    string location = 9;
    // The user-specified Cloud Dataflow job name.
    // 
    // Only one Job with a given name may exist in a project at any
    // given time. If a caller attempts to create a Job with the same
    // name as an already-existing Job, the attempt returns the
    // existing Job.
    // 
    // The name must match the regular expression
    // `[a-z]([-a-z0-9]{0,38}[a-z0-9])?`
    string name = 10;
    // Preliminary field: The format of this data may change at any time.
    // A description of the user pipeline and stages through which it is executed.
    // Created by Cloud Dataflow service.  Only retrieved with
    // JOB_VIEW_DESCRIPTION or JOB_VIEW_ALL.
    PipelineDescription pipelineDescription = 11;
    // The ID of the Cloud Platform project that the job belongs to.
    string projectId = 12;
    // If this job is an update of an existing job, this field is the job ID
    // of the job it replaced.
    // 
    // When sending a `CreateJobRequest`, you can update a job by specifying it
    // here. The job named here is stopped, and its intermediate state is
    // transferred to this job.
    string replaceJobId = 13;
    // If another job is an update of this job (and thus, this job is in
    // `JOB_STATE_UPDATED`), this field contains the ID of that job.
    string replacedByJobId = 14;
    // The job's requested state.
    // 
    // `UpdateJob` may be used to switch between the `JOB_STATE_STOPPED` and
    // `JOB_STATE_RUNNING` states, by setting requested_state.  `UpdateJob` may
    // also be used to directly set a job's requested state to
    // `JOB_STATE_CANCELLED` or `JOB_STATE_DONE`, irrevocably terminating the
    // job if it has not already reached a terminal state.
    enum Job_RequestedState {
        JOB_REQUESTEDSTATE_JOB_STATE_UNKNOWN = 0;
        JOB_REQUESTEDSTATE_JOB_STATE_STOPPED = 1;
        JOB_REQUESTEDSTATE_JOB_STATE_RUNNING = 2;
        JOB_REQUESTEDSTATE_JOB_STATE_DONE = 3;
        JOB_REQUESTEDSTATE_JOB_STATE_FAILED = 4;
        JOB_REQUESTEDSTATE_JOB_STATE_CANCELLED = 5;
        JOB_REQUESTEDSTATE_JOB_STATE_UPDATED = 6;
        JOB_REQUESTEDSTATE_JOB_STATE_DRAINING = 7;
        JOB_REQUESTEDSTATE_JOB_STATE_DRAINED = 8;
        JOB_REQUESTEDSTATE_JOB_STATE_PENDING = 9;
        JOB_REQUESTEDSTATE_JOB_STATE_CANCELLING = 10;
    }
    Job_RequestedState requestedState = 15;
    // This field may be mutated by the Cloud Dataflow service;
    // callers cannot mutate it.
    repeated ExecutionStageState stageStates = 16;
    // The top-level steps that constitute the entire job.
    repeated Step steps = 17;
    // A set of files the system should be aware of that are used
    // for temporary storage. These temporary files will be
    // removed on job completion.
    // No duplicates are allowed.
    // No file patterns are supported.
    // 
    // The supported files are:
    // 
    // Google Cloud Storage:
    // 
    //    storage.googleapis.com/{bucket}/{object}
    //    bucket.storage.googleapis.com/{object}
    repeated string tempFiles = 18;
    // The map of transform name prefixes of the job to be replaced to the
    // corresponding name prefixes of the new job.
    map<string, string> transformNameMapping = 19;
    // The type of Cloud Dataflow job.
    enum Job_Type {
        JOB_TYPE_JOB_TYPE_UNKNOWN = 0;
        JOB_TYPE_JOB_TYPE_BATCH = 1;
        JOB_TYPE_JOB_TYPE_STREAMING = 2;
    }
    Job_Type type = 20;
}

message JobExecutionInfo {
    // A mapping from each stage to the information about that stage.
    map<string, JobExecutionStageInfo> stages = 1;
}

message JobExecutionStageInfo {
    // The steps associated with the execution stage.
    // Note that stages may have several steps, and that a given step
    // might be run by more than one stage.
    repeated string stepName = 1;
}

message JobMessage {
    // Deprecated.
    string id = 1;
    // Importance level of the message.
    enum JobMessage_MessageImportance {
        JOBMESSAGE_MESSAGEIMPORTANCE_JOB_MESSAGE_IMPORTANCE_UNKNOWN = 0;
        JOBMESSAGE_MESSAGEIMPORTANCE_JOB_MESSAGE_DEBUG = 1;
        JOBMESSAGE_MESSAGEIMPORTANCE_JOB_MESSAGE_DETAILED = 2;
        JOBMESSAGE_MESSAGEIMPORTANCE_JOB_MESSAGE_BASIC = 3;
        JOBMESSAGE_MESSAGEIMPORTANCE_JOB_MESSAGE_WARNING = 4;
        JOBMESSAGE_MESSAGEIMPORTANCE_JOB_MESSAGE_ERROR = 5;
    }
    JobMessage_MessageImportance messageImportance = 2;
    // The text of the message.
    string messageText = 3;
    // The timestamp of the message.
    string time = 4;
}

message JobMetrics {
    // Timestamp as of which metric values are current.
    string metricTime = 1;
    // All metrics for this job.
    repeated MetricUpdate metrics = 2;
}

message KeyRangeDataDiskAssignment {
    // The name of the data disk where data for this range is stored.
    // This name is local to the Google Cloud Platform project and uniquely
    // identifies the disk within that project, for example
    // "myproject-1014-104817-4c2-harness-0-disk-1".
    string dataDisk = 1;
    // The end (exclusive) of the key range.
    string end = 2;
    // The start (inclusive) of the key range.
    string start = 3;
}

message KeyRangeLocation {
    // The name of the data disk where data for this range is stored.
    // This name is local to the Google Cloud Platform project and uniquely
    // identifies the disk within that project, for example
    // "myproject-1014-104817-4c2-harness-0-disk-1".
    string dataDisk = 1;
    // The physical location of this range assignment to be used for
    // streaming computation cross-worker message delivery.
    string deliveryEndpoint = 2;
    // DEPRECATED. The location of the persistent state for this range, as a
    // persistent directory in the worker local filesystem.
    string deprecatedPersistentDirectory = 3;
    // The end (exclusive) of the key range.
    string end = 4;
    // The start (inclusive) of the key range.
    string start = 5;
}

message LaunchTemplateParameters {
    // The runtime environment for the job.
    RuntimeEnvironment environment = 1;
    // Required. The job name to use for the created job.
    string jobName = 2;
    // The runtime parameters to pass to the job.
    map<string, string> parameters = 3;
}

message LaunchTemplateResponse {
    // The job that was launched, if the request was not a dry run and
    // the job was successfully launched.
    Job job = 1;
}

message LeaseWorkItemRequest {
    // The current timestamp at the worker.
    string currentWorkerTime = 1;
    // The location which contains the WorkItem's job.
    string location = 2;
    // The initial lease period.
    string requestedLeaseDuration = 3;
    // Filter for WorkItem type.
    repeated string workItemTypes = 4;
    // Worker capabilities. WorkItems might be limited to workers with specific
    // capabilities.
    repeated string workerCapabilities = 5;
    // Identifies the worker leasing work -- typically the ID of the
    // virtual machine running the worker.
    string workerId = 6;
}

message LeaseWorkItemResponse {
    // A list of the leased WorkItems.
    repeated WorkItem workItems = 1;
}

message ListJobMessagesResponse {
    // Autoscaling events in ascending timestamp order.
    repeated AutoscalingEvent autoscalingEvents = 1;
    // Messages in ascending timestamp order.
    repeated JobMessage jobMessages = 2;
    // The token to obtain the next page of results if there are more.
    string nextPageToken = 3;
}

message ListJobsResponse {
    // Zero or more messages describing locations that failed to respond.
    repeated FailedLocation failedLocation = 1;
    // A subset of the requested job information.
    repeated Job jobs = 2;
    // Set if there may be more results than fit in this response.
    string nextPageToken = 3;
}

message MapTask {
    // The instructions in the MapTask.
    repeated ParallelInstruction instructions = 1;
    // System-defined name of the stage containing this MapTask.
    // Unique across the workflow.
    string stageName = 2;
    // System-defined name of this MapTask.
    // Unique across the workflow.
    string systemName = 3;
}

message MetricShortId {
    // The index of the corresponding metric in
    // the ReportWorkItemStatusRequest. Required.
    int32 metricIndex = 1;
    // The service-generated short identifier for the metric.
    string shortId = 2;
}

message MetricStructuredName {
    // Zero or more labeled fields which identify the part of the job this
    // metric is associated with, such as the name of a step or collection.
    // 
    // For example, built-in counters associated with steps will have
    // context['step'] = <step-name>. Counters associated with PCollections
    // in the SDK will have context['pcollection'] = <pcollection-name>.
    map<string, string> context = 1;
    // Worker-defined metric name.
    string name = 2;
    // Origin (namespace) of metric name. May be blank for user-define metrics;
    // will be "dataflow" for metrics defined by the Dataflow service or SDK.
    string origin = 3;
}

message MetricUpdate {
    // True if this metric is reported as the total cumulative aggregate
    // value accumulated since the worker started working on this WorkItem.
    // By default this is false, indicating that this metric is reported
    // as a delta that is not associated with any WorkItem.
    bool cumulative = 1;
    // A struct value describing properties of a distribution of numeric values.
    ;
    // Worker-computed aggregate value for internal use by the Dataflow
    // service.
    ;
    // Metric aggregation kind.  The possible metric aggregation kinds are
    // "Sum", "Max", "Min", "Mean", "Set", "And", "Or", and "Distribution".
    // The specified aggregation kind is case-insensitive.
    // 
    // If omitted, this is not an aggregated value but instead
    // a single metric sample value.
    string kind = 4;
    // Worker-computed aggregate value for the "Mean" aggregation kind.
    // This holds the count of the aggregated values and is used in combination
    // with mean_sum above to obtain the actual mean aggregate value.
    // The only possible value type is Long.
    ;
    // Worker-computed aggregate value for the "Mean" aggregation kind.
    // This holds the sum of the aggregated values and is used in combination
    // with mean_count below to obtain the actual mean aggregate value.
    // The only possible value types are Long and Double.
    ;
    // Name of the metric.
    MetricStructuredName name = 7;
    // Worker-computed aggregate value for aggregation kinds "Sum", "Max", "Min",
    // "And", and "Or".  The possible value types are Long, Double, and Boolean.
    ;
    // Worker-computed aggregate value for the "Set" aggregation kind.  The only
    // possible value type is a list of Values whose type can be Long, Double,
    // or String, according to the metric's type.  All Values in the list must
    // be of the same type.
    ;
    // Timestamp associated with the metric value. Optional when workers are
    // reporting work progress; it will be filled in responses from the
    // metrics API.
    string updateTime = 10;
}

message MountedDataDisk {
    // The name of the data disk.
    // This name is local to the Google Cloud Platform project and uniquely
    // identifies the disk within that project, for example
    // "myproject-1014-104817-4c2-harness-0-disk-1".
    string dataDisk = 1;
}

message MultiOutputInfo {
    // The id of the tag the user code will emit to this output by; this
    // should correspond to the tag of some SideInputInfo.
    string tag = 1;
}

message NameAndKind {
    // Counter aggregation kind.
    enum NameAndKind_Kind {
        NAMEANDKIND_KIND_INVALID = 0;
        NAMEANDKIND_KIND_SUM = 1;
        NAMEANDKIND_KIND_MAX = 2;
        NAMEANDKIND_KIND_MIN = 3;
        NAMEANDKIND_KIND_MEAN = 4;
        NAMEANDKIND_KIND_OR = 5;
        NAMEANDKIND_KIND_AND = 6;
        NAMEANDKIND_KIND_SET = 7;
        NAMEANDKIND_KIND_DISTRIBUTION = 8;
    }
    NameAndKind_Kind kind = 1;
    // Name of the counter.
    string name = 2;
}

message Package {
    // The resource to read the package from. The supported resource type is:
    // 
    // Google Cloud Storage:
    // 
    //   storage.googleapis.com/{bucket}
    //   bucket.storage.googleapis.com
    string location = 1;
    // The name of the package.
    string name = 2;
}

message ParDoInstruction {
    // The input.
    InstructionInput input = 1;
    // Information about each of the outputs, if user_fn is a  MultiDoFn.
    repeated MultiOutputInfo multiOutputInfos = 2;
    // The number of outputs.
    int32 numOutputs = 3;
    // Zero or more side inputs.
    repeated SideInputInfo sideInputs = 4;
    // The user function to invoke.
    map<string, > userFn = 5;
}

message ParallelInstruction {
    // Additional information for Flatten instructions.
    FlattenInstruction flatten = 1;
    // User-provided name of this operation.
    string name = 2;
    // System-defined name for the operation in the original workflow graph.
    string originalName = 3;
    // Describes the outputs of the instruction.
    repeated InstructionOutput outputs = 4;
    // Additional information for ParDo instructions.
    ParDoInstruction parDo = 5;
    // Additional information for PartialGroupByKey instructions.
    PartialGroupByKeyInstruction partialGroupByKey = 6;
    // Additional information for Read instructions.
    ReadInstruction read = 7;
    // System-defined name of this operation.
    // Unique across the workflow.
    string systemName = 8;
    // Additional information for Write instructions.
    WriteInstruction write = 9;
}

message Parameter {
    // Key or name for this parameter.
    string key = 1;
    // Value for this parameter.
    ;
}

message ParameterMetadata {
    // Required. The help text to display for the parameter.
    string helpText = 1;
    // Optional. Whether the parameter is optional. Defaults to false.
    bool isOptional = 2;
    // Required. The label to display for the parameter.
    string label = 3;
    // Required. The name of the parameter.
    string name = 4;
    // Optional. Regexes that the parameter must match.
    repeated string regexes = 5;
}

message PartialGroupByKeyInstruction {
    // Describes the input to the partial group-by-key instruction.
    InstructionInput input = 1;
    // The codec to use for interpreting an element in the input PTable.
    map<string, > inputElementCodec = 2;
    // If this instruction includes a combining function this is the name of the
    // intermediate store between the GBK and the CombineValues.
    string originalCombineValuesInputStoreName = 3;
    // If this instruction includes a combining function, this is the name of the
    // CombineValues instruction lifted into this instruction.
    string originalCombineValuesStepName = 4;
    // Zero or more side inputs.
    repeated SideInputInfo sideInputs = 5;
    // The value combining function to invoke.
    map<string, > valueCombiningFn = 6;
}

message PipelineDescription {
    // Pipeline level display data.
    repeated DisplayData displayData = 1;
    // Description of each stage of execution of the pipeline.
    repeated ExecutionStageSummary executionPipelineStage = 2;
    // Description of each transform in the pipeline and collections between them.
    repeated TransformSummary originalPipelineTransform = 3;
}

message Position {
    // Position is a byte offset.
    string byteOffset = 1;
    // CloudPosition is a concat position.
    ConcatPosition concatPosition = 2;
    // Position is past all other positions. Also useful for the end
    // position of an unbounded range.
    bool end = 3;
    // Position is a string key, ordered lexicographically.
    string key = 4;
    // Position is a record index.
    string recordIndex = 5;
    // CloudPosition is a base64 encoded BatchShufflePosition (with FIXED
    // sharding).
    string shufflePosition = 6;
}

message PubsubLocation {
    // Indicates whether the pipeline allows late-arriving data.
    bool dropLateData = 1;
    // If set, contains a pubsub label from which to extract record ids.
    // If left empty, record deduplication will be strictly best effort.
    string idLabel = 2;
    // A pubsub subscription, in the form of
    // "pubsub.googleapis.com/subscriptions/<project-id>/<subscription-name>"
    string subscription = 3;
    // If set, contains a pubsub label from which to extract record timestamps.
    // If left empty, record timestamps will be generated upon arrival.
    string timestampLabel = 4;
    // A pubsub topic, in the form of
    // "pubsub.googleapis.com/topics/<project-id>/<topic-name>"
    string topic = 5;
    // If set, specifies the pubsub subscription that will be used for tracking
    // custom time timestamps for watermark estimation.
    string trackingSubscription = 6;
    // If true, then the client has requested to get pubsub attributes.
    bool withAttributes = 7;
}

message ReadInstruction {
    // The source to read from.
    Source source = 1;
}

message ReportWorkItemStatusRequest {
    // The current timestamp at the worker.
    string currentWorkerTime = 1;
    // The location which contains the WorkItem's job.
    string location = 2;
    // The order is unimportant, except that the order of the
    // WorkItemServiceState messages in the ReportWorkItemStatusResponse
    // corresponds to the order of WorkItemStatus messages here.
    repeated WorkItemStatus workItemStatuses = 3;
    // The ID of the worker reporting the WorkItem status.  If this
    // does not match the ID of the worker which the Dataflow service
    // believes currently has the lease on the WorkItem, the report
    // will be dropped (with an error response).
    string workerId = 4;
}

message ReportWorkItemStatusResponse {
    // A set of messages indicating the service-side state for each
    // WorkItem whose status was reported, in the same order as the
    // WorkItemStatus messages in the ReportWorkItemStatusRequest which
    // resulting in this response.
    repeated WorkItemServiceState workItemServiceStates = 1;
}

message ReportedParallelism {
    // Specifies whether the parallelism is infinite. If true, "value" is
    // ignored.
    // Infinite parallelism means the service will assume that the work item
    // can always be split into more non-empty work items by dynamic splitting.
    // This is a work-around for lack of support for infinity by the current
    // JSON-based Java RPC stack.
    bool isInfinite = 1;
    // Specifies the level of parallelism in case it is finite.
    double value = 2;
}

message ResourceUtilizationReport {
    // CPU utilization samples.
    repeated CPUTime cpuTime = 1;
}

message ResourceUtilizationReportResponse {
}

message RuntimeEnvironment {
    // Whether to bypass the safety checks for the job's temporary directory.
    // Use with caution.
    bool bypassTempDirValidation = 1;
    // The machine type to use for the job. Defaults to the value from the
    // template if not specified.
    string machineType = 2;
    // The maximum number of Google Compute Engine instances to be made
    // available to your pipeline during execution, from 1 to 1000.
    int32 maxWorkers = 3;
    // The email address of the service account to run the job as.
    string serviceAccountEmail = 4;
    // The Cloud Storage path to use for temporary files.
    // Must be a valid Cloud Storage URL, beginning with `gs://`.
    string tempLocation = 5;
    // The Compute Engine [availability
    // zone](https://cloud.google.com/compute/docs/regions-zones/regions-zones)
    // for launching worker instances to run your pipeline.
    string zone = 6;
}

message SendDebugCaptureRequest {
    // The internal component id for which debug information is sent.
    string componentId = 1;
    // The encoded debug information.
    string data = 2;
    // The location which contains the job specified by job_id.
    string location = 3;
    // The worker id, i.e., VM hostname.
    string workerId = 4;
}

message SendDebugCaptureResponse {
}

message SendWorkerMessagesRequest {
    // The location which contains the job
    string location = 1;
    // The WorkerMessages to send.
    repeated WorkerMessage workerMessages = 2;
}

message SendWorkerMessagesResponse {
    // The servers response to the worker messages.
    repeated WorkerMessageResponse workerMessageResponses = 1;
}

message SeqMapTask {
    // Information about each of the inputs.
    repeated SideInputInfo inputs = 1;
    // The user-provided name of the SeqDo operation.
    string name = 2;
    // Information about each of the outputs.
    repeated SeqMapTaskOutputInfo outputInfos = 3;
    // System-defined name of the stage containing the SeqDo operation.
    // Unique across the workflow.
    string stageName = 4;
    // System-defined name of the SeqDo operation.
    // Unique across the workflow.
    string systemName = 5;
    // The user function to invoke.
    map<string, > userFn = 6;
}

message SeqMapTaskOutputInfo {
    // The sink to write the output value to.
    Sink sink = 1;
    // The id of the TupleTag the user code will tag the output value by.
    string tag = 2;
}

message ShellTask {
    // The shell command to run.
    string command = 1;
    // Exit code for the task.
    int32 exitCode = 2;
}

message SideInputId {
    // The step that receives and usually consumes this side input.
    string declaringStepName = 1;
    // The index of the side input, from the list of non_parallel_inputs.
    int32 inputIndex = 2;
}

message SideInputInfo {
    // How to interpret the source element(s) as a side input value.
    map<string, > kind = 1;
    // The source(s) to read element(s) from to get the value of this side input.
    // If more than one source, then the elements are taken from the
    // sources, in the specified order if order matters.
    // At least one source is required.
    repeated Source sources = 2;
    // The id of the tag the user code will access this side input by;
    // this should correspond to the tag of some MultiOutputInfo.
    string tag = 3;
}

message Sink {
    // The codec to use to encode data written to the sink.
    map<string, > codec = 1;
    // The sink to write to, plus its parameters.
    map<string, > spec = 2;
}

message Source {
    // While splitting, sources may specify the produced bundles
    // as differences against another source, in order to save backend-side
    // memory and allow bigger jobs. For details, see SourceSplitRequest.
    // To support this use case, the full set of parameters of the source
    // is logically obtained by taking the latest explicitly specified value
    // of each parameter in the order:
    // base_specs (later items win), spec (overrides anything in base_specs).
    message BaseSpec {
    }
    repeated BaseSpec baseSpecs = 1;
    // The codec to use to decode data read from the source.
    map<string, > codec = 2;
    // Setting this value to true hints to the framework that the source
    // doesn't need splitting, and using SourceSplitRequest on it would
    // yield SOURCE_SPLIT_OUTCOME_USE_CURRENT.
    // 
    // E.g. a file splitter may set this to true when splitting a single file
    // into a set of byte ranges of appropriate size, and set this
    // to false when splitting a filepattern into individual files.
    // However, for efficiency, a file splitter may decide to produce
    // file subranges directly from the filepattern to avoid a splitting
    // round-trip.
    // 
    // See SourceSplitRequest for an overview of the splitting process.
    // 
    // This field is meaningful only in the Source objects populated
    // by the user (e.g. when filling in a DerivedSource).
    // Source objects supplied by the framework to the user don't have
    // this field populated.
    bool doesNotNeedSplitting = 3;
    // Optionally, metadata for this source can be supplied right away,
    // avoiding a SourceGetMetadataOperation roundtrip
    // (see SourceOperationRequest).
    // 
    // This field is meaningful only in the Source objects populated
    // by the user (e.g. when filling in a DerivedSource).
    // Source objects supplied by the framework to the user don't have
    // this field populated.
    SourceMetadata metadata = 4;
    // The source to read from, plus its parameters.
    map<string, > spec = 5;
}

message SourceFork {
    // DEPRECATED
    SourceSplitShard primary = 1;
    // DEPRECATED
    DerivedSource primarySource = 2;
    // DEPRECATED
    SourceSplitShard residual = 3;
    // DEPRECATED
    DerivedSource residualSource = 4;
}

message SourceGetMetadataRequest {
    // Specification of the source whose metadata should be computed.
    Source source = 1;
}

message SourceGetMetadataResponse {
    // The computed metadata.
    SourceMetadata metadata = 1;
}

message SourceMetadata {
    // An estimate of the total size (in bytes) of the data that would be
    // read from this source.  This estimate is in terms of external storage
    // size, before any decompression or other processing done by the reader.
    string estimatedSizeBytes = 1;
    // Specifies that the size of this source is known to be infinite
    // (this is a streaming source).
    bool infinite = 2;
    // Whether this source is known to produce key/value pairs with
    // the (encoded) keys in lexicographically sorted order.
    bool producesSortedKeys = 3;
}

message SourceOperationRequest {
    // Information about a request to get metadata about a source.
    SourceGetMetadataRequest getMetadata = 1;
    // Information about a request to split a source.
    SourceSplitRequest split = 2;
}

message SourceOperationResponse {
    // A response to a request to get metadata about a source.
    SourceGetMetadataResponse getMetadata = 1;
    // A response to a request to split a source.
    SourceSplitResponse split = 2;
}

message SourceSplitOptions {
    // The source should be split into a set of bundles where the estimated size
    // of each is approximately this many bytes.
    string desiredBundleSizeBytes = 1;
    // DEPRECATED in favor of desired_bundle_size_bytes.
    string desiredShardSizeBytes = 2;
}

message SourceSplitRequest {
    // Hints for tuning the splitting process.
    SourceSplitOptions options = 1;
    // Specification of the source to be split.
    Source source = 2;
}

message SourceSplitResponse {
    // If outcome is SPLITTING_HAPPENED, then this is a list of bundles
    // into which the source was split. Otherwise this field is ignored.
    // This list can be empty, which means the source represents an empty input.
    repeated DerivedSource bundles = 1;
    // Indicates whether splitting happened and produced a list of bundles.
    // If this is USE_CURRENT_SOURCE_AS_IS, the current source should
    // be processed "as is" without splitting. "bundles" is ignored in this case.
    // If this is SPLITTING_HAPPENED, then "bundles" contains a list of
    // bundles into which the source was split.
    enum SourceSplitResponse_Outcome {
        SOURCESPLITRESPONSE_OUTCOME_SOURCE_SPLIT_OUTCOME_UNKNOWN = 0;
        SOURCESPLITRESPONSE_OUTCOME_SOURCE_SPLIT_OUTCOME_USE_CURRENT = 1;
        SOURCESPLITRESPONSE_OUTCOME_SOURCE_SPLIT_OUTCOME_SPLITTING_HAPPENED = 2;
    }
    SourceSplitResponse_Outcome outcome = 2;
    // DEPRECATED in favor of bundles.
    repeated SourceSplitShard shards = 3;
}

message SourceSplitShard {
    // DEPRECATED
    enum SourceSplitShard_DerivationMode {
        SOURCESPLITSHARD_DERIVATIONMODE_SOURCE_DERIVATION_MODE_UNKNOWN = 0;
        SOURCESPLITSHARD_DERIVATIONMODE_SOURCE_DERIVATION_MODE_INDEPENDENT = 1;
        SOURCESPLITSHARD_DERIVATIONMODE_SOURCE_DERIVATION_MODE_CHILD_OF_CURRENT = 2;
        SOURCESPLITSHARD_DERIVATIONMODE_SOURCE_DERIVATION_MODE_SIBLING_OF_CURRENT = 3;
    }
    SourceSplitShard_DerivationMode derivationMode = 1;
    // DEPRECATED
    Source source = 2;
}

message SplitInt64 {
    // The high order bits, including the sign: n >> 32.
    int32 highBits = 1;
    // The low order bits: n & 0xffffffff.
    uint32 lowBits = 2;
}

message StageSource {
    // Dataflow service generated name for this source.
    string name = 1;
    // User name for the original user transform or collection with which this
    // source is most closely associated.
    string originalTransformOrCollection = 2;
    // Size of the source, if measurable.
    string sizeBytes = 3;
    // Human-readable name for this source; may be user or system generated.
    string userName = 4;
}

message StateFamilyConfig {
    // If true, this family corresponds to a read operation.
    bool isRead = 1;
    // The state family value.
    string stateFamily = 2;
}

message Status {
    // The status code, which should be an enum value of google.rpc.Code.
    int32 code = 1;
    // A list of messages that carry the error details.  There is a common set of
    // message types for APIs to use.
    message Detail {
    }
    repeated Detail details = 2;
    // A developer-facing error message, which should be in English. Any
    // user-facing error message should be localized and sent in the
    // google.rpc.Status.details field, or localized by the client.
    string message = 3;
}

message Step {
    // The kind of step in the Cloud Dataflow job.
    string kind = 1;
    // The name that identifies the step. This must be unique for each
    // step with respect to all other steps in the Cloud Dataflow job.
    string name = 2;
    // Named properties associated with the step. Each kind of
    // predefined step has its own required set of properties.
    // Must be provided on Create.  Only retrieved with JOB_VIEW_ALL.
    map<string, > properties = 3;
}

message StreamLocation {
    // The stream is a custom source.
    CustomSourceLocation customSourceLocation = 1;
    // The stream is a pubsub stream.
    PubsubLocation pubsubLocation = 2;
    // The stream is a streaming side input.
    StreamingSideInputLocation sideInputLocation = 3;
    // The stream is part of another computation within the current
    // streaming Dataflow job.
    StreamingStageLocation streamingStageLocation = 4;
}

message StreamingComputationConfig {
    // Unique identifier for this computation.
    string computationId = 1;
    // Instructions that comprise the computation.
    repeated ParallelInstruction instructions = 2;
    // Stage name of this computation.
    string stageName = 3;
    // System defined name for this computation.
    string systemName = 4;
}

message StreamingComputationRanges {
    // The ID of the computation.
    string computationId = 1;
    // Data disk assignments for ranges from this computation.
    repeated KeyRangeDataDiskAssignment rangeAssignments = 2;
}

message StreamingComputationTask {
    // Contains ranges of a streaming computation this task should apply to.
    repeated StreamingComputationRanges computationRanges = 1;
    // Describes the set of data disks this task should apply to.
    repeated MountedDataDisk dataDisks = 2;
    // A type of streaming computation task.
    enum StreamingComputationTask_TaskType {
        STREAMINGCOMPUTATIONTASK_TASKTYPE_STREAMING_COMPUTATION_TASK_UNKNOWN = 0;
        STREAMINGCOMPUTATIONTASK_TASKTYPE_STREAMING_COMPUTATION_TASK_STOP = 1;
        STREAMINGCOMPUTATIONTASK_TASKTYPE_STREAMING_COMPUTATION_TASK_START = 2;
    }
    StreamingComputationTask_TaskType taskType = 3;
}

message StreamingConfigTask {
    // Set of computation configuration information.
    repeated StreamingComputationConfig streamingComputationConfigs = 1;
    // Map from user step names to state families.
    map<string, string> userStepToStateFamilyNameMap = 2;
    // If present, the worker must use this endpoint to communicate with Windmill
    // Service dispatchers, otherwise the worker must continue to use whatever
    // endpoint it had been using.
    string windmillServiceEndpoint = 3;
    // If present, the worker must use this port to communicate with Windmill
    // Service dispatchers. Only applicable when windmill_service_endpoint is
    // specified.
    string windmillServicePort = 4;
}

message StreamingSetupTask {
    // The user has requested drain.
    bool drain = 1;
    // The TCP port on which the worker should listen for messages from
    // other streaming computation workers.
    int32 receiveWorkPort = 2;
    // The global topology of the streaming Dataflow job.
    TopologyConfig streamingComputationTopology = 3;
    // The TCP port used by the worker to communicate with the Dataflow
    // worker harness.
    int32 workerHarnessPort = 4;
}

message StreamingSideInputLocation {
    // Identifies the state family where this side input is stored.
    string stateFamily = 1;
    // Identifies the particular side input within the streaming Dataflow job.
    string tag = 2;
}

message StreamingStageLocation {
    // Identifies the particular stream within the streaming Dataflow
    // job.
    string streamId = 1;
}

message StringList {
    // Elements of the list.
    repeated string elements = 1;
}

message StructuredMessage {
    // Idenfier for this message type.  Used by external systems to
    // internationalize or personalize message.
    string messageKey = 1;
    // Human-readable version of message.
    string messageText = 2;
    // The structured data associated with this message.
    repeated Parameter parameters = 3;
}

message TaskRunnerSettings {
    // Whether to also send taskrunner log info to stderr.
    bool alsologtostderr = 1;
    // The location on the worker for task-specific subdirectories.
    string baseTaskDir = 2;
    // The base URL for the taskrunner to use when accessing Google Cloud APIs.
    // 
    // When workers access Google Cloud APIs, they logically do so via
    // relative URLs.  If this field is specified, it supplies the base
    // URL to use for resolving these relative URLs.  The normative
    // algorithm used is defined by RFC 1808, "Relative Uniform Resource
    // Locators".
    // 
    // If not specified, the default value is "http://www.googleapis.com/"
    string baseUrl = 3;
    // The file to store preprocessing commands in.
    string commandlinesFileName = 4;
    // Whether to continue taskrunner if an exception is hit.
    bool continueOnException = 5;
    // The API version of endpoint, e.g. "v1b3"
    string dataflowApiVersion = 6;
    // The command to launch the worker harness.
    string harnessCommand = 7;
    // The suggested backend language.
    string languageHint = 8;
    // The directory on the VM to store logs.
    string logDir = 9;
    // Whether to send taskrunner log info to Google Compute Engine VM serial
    // console.
    bool logToSerialconsole = 10;
    // Indicates where to put logs.  If this is not specified, the logs
    // will not be uploaded.
    // 
    // The supported resource type is:
    // 
    // Google Cloud Storage:
    //   storage.googleapis.com/{bucket}/{object}
    //   bucket.storage.googleapis.com/{object}
    string logUploadLocation = 11;
    // The OAuth2 scopes to be requested by the taskrunner in order to
    // access the Cloud Dataflow API.
    repeated string oauthScopes = 12;
    // The settings to pass to the parallel worker harness.
    WorkerSettings parallelWorkerSettings = 13;
    // The streaming worker main class name.
    string streamingWorkerMainClass = 14;
    // The UNIX group ID on the worker VM to use for tasks launched by
    // taskrunner; e.g. "wheel".
    string taskGroup = 15;
    // The UNIX user ID on the worker VM to use for tasks launched by
    // taskrunner; e.g. "root".
    string taskUser = 16;
    // The prefix of the resources the taskrunner should use for
    // temporary storage.
    // 
    // The supported resource type is:
    // 
    // Google Cloud Storage:
    //   storage.googleapis.com/{bucket}/{object}
    //   bucket.storage.googleapis.com/{object}
    string tempStoragePrefix = 17;
    // The ID string of the VM.
    string vmId = 18;
    // The file to store the workflow in.
    string workflowFileName = 19;
}

message TemplateMetadata {
    // Optional. A description of the template.
    string description = 1;
    // Required. The name of the template.
    string name = 2;
    // The parameters for the template.
    repeated ParameterMetadata parameters = 3;
}

message TopologyConfig {
    // The computations associated with a streaming Dataflow job.
    repeated ComputationTopology computations = 1;
    // The disks assigned to a streaming Dataflow job.
    repeated DataDiskAssignment dataDiskAssignments = 2;
    // The size (in bits) of keys that will be assigned to source messages.
    int32 forwardingKeyBits = 3;
    // Version number for persistent state.
    int32 persistentStateVersion = 4;
    // Maps user stage names to stable computation names.
    map<string, string> userStageToComputationNameMap = 5;
}

message TransformSummary {
    // Transform-specific display data.
    repeated DisplayData displayData = 1;
    // SDK generated id of this transform instance.
    string id = 2;
    // User names for all collection inputs to this transform.
    repeated string inputCollectionName = 3;
    // Type of transform.
    enum TransformSummary_Kind {
        TRANSFORMSUMMARY_KIND_UNKNOWN_KIND = 0;
        TRANSFORMSUMMARY_KIND_PAR_DO_KIND = 1;
        TRANSFORMSUMMARY_KIND_GROUP_BY_KEY_KIND = 2;
        TRANSFORMSUMMARY_KIND_FLATTEN_KIND = 3;
        TRANSFORMSUMMARY_KIND_READ_KIND = 4;
        TRANSFORMSUMMARY_KIND_WRITE_KIND = 5;
        TRANSFORMSUMMARY_KIND_CONSTANT_KIND = 6;
        TRANSFORMSUMMARY_KIND_SINGLETON_KIND = 7;
        TRANSFORMSUMMARY_KIND_SHUFFLE_KIND = 8;
    }
    TransformSummary_Kind kind = 4;
    // User provided name for this transform instance.
    string name = 5;
    // User  names for all collection outputs to this transform.
    repeated string outputCollectionName = 6;
}

message WorkItem {
    // Work item-specific configuration as an opaque blob.
    string configuration = 1;
    // Identifies this WorkItem.
    string id = 2;
    // The initial index to use when reporting the status of the WorkItem.
    string initialReportIndex = 3;
    // Identifies the workflow job this WorkItem belongs to.
    string jobId = 4;
    // Time when the lease on this Work will expire.
    string leaseExpireTime = 5;
    // Additional information for MapTask WorkItems.
    MapTask mapTask = 6;
    // Any required packages that need to be fetched in order to execute
    // this WorkItem.
    repeated Package packages = 7;
    // Identifies the cloud project this WorkItem belongs to.
    string projectId = 8;
    // Recommended reporting interval.
    string reportStatusInterval = 9;
    // Additional information for SeqMapTask WorkItems.
    SeqMapTask seqMapTask = 10;
    // Additional information for ShellTask WorkItems.
    ShellTask shellTask = 11;
    // Additional information for source operation WorkItems.
    SourceOperationRequest sourceOperationTask = 12;
    // Additional information for StreamingComputationTask WorkItems.
    StreamingComputationTask streamingComputationTask = 13;
    // Additional information for StreamingConfigTask WorkItems.
    StreamingConfigTask streamingConfigTask = 14;
    // Additional information for StreamingSetupTask WorkItems.
    StreamingSetupTask streamingSetupTask = 15;
}

message WorkItemServiceState {
    // Other data returned by the service, specific to the particular
    // worker harness.
    map<string, > harnessData = 1;
    // Time at which the current lease will expire.
    string leaseExpireTime = 2;
    // The short ids that workers should use in subsequent metric updates.
    // Workers should strive to use short ids whenever possible, but it is ok
    // to request the short_id again if a worker lost track of it
    // (e.g. if the worker is recovering from a crash).
    // NOTE: it is possible that the response may have short ids for a subset
    // of the metrics.
    repeated MetricShortId metricShortId = 3;
    // The index value to use for the next report sent by the worker.
    // Note: If the report call fails for whatever reason, the worker should
    // reuse this index for subsequent report attempts.
    string nextReportIndex = 4;
    // New recommended reporting interval.
    string reportStatusInterval = 5;
    // The progress point in the WorkItem where the Dataflow service
    // suggests that the worker truncate the task.
    ApproximateSplitRequest splitRequest = 6;
    // DEPRECATED in favor of split_request.
    ApproximateProgress suggestedStopPoint = 7;
    // Obsolete, always empty.
    Position suggestedStopPosition = 8;
}

message WorkItemStatus {
    // True if the WorkItem was completed (successfully or unsuccessfully).
    bool completed = 1;
    // Worker output counters for this WorkItem.
    repeated CounterUpdate counterUpdates = 2;
    // See documentation of stop_position.
    DynamicSourceSplit dynamicSourceSplit = 3;
    // Specifies errors which occurred during processing.  If errors are
    // provided, and completed = true, then the WorkItem is considered
    // to have failed.
    repeated Status errors = 4;
    // DEPRECATED in favor of counter_updates.
    repeated MetricUpdate metricUpdates = 5;
    // DEPRECATED in favor of reported_progress.
    ApproximateProgress progress = 6;
    // The report index.  When a WorkItem is leased, the lease will
    // contain an initial report index.  When a WorkItem's status is
    // reported to the system, the report should be sent with
    // that report index, and the response will contain the index the
    // worker should use for the next report.  Reports received with
    // unexpected index values will be rejected by the service.
    // 
    // In order to preserve idempotency, the worker should not alter the
    // contents of a report, even if the worker must submit the same
    // report multiple times before getting back a response.  The worker
    // should not submit a subsequent report until the response for the
    // previous report had been received from the service.
    string reportIndex = 7;
    // The worker's progress through this WorkItem.
    ApproximateReportedProgress reportedProgress = 8;
    // Amount of time the worker requests for its lease.
    string requestedLeaseDuration = 9;
    // DEPRECATED in favor of dynamic_source_split.
    SourceFork sourceFork = 10;
    // If the work item represented a SourceOperationRequest, and the work
    // is completed, contains the result of the operation.
    SourceOperationResponse sourceOperationResponse = 11;
    // A worker may split an active map task in two parts, "primary" and
    // "residual", continuing to process the primary part and returning the
    // residual part into the pool of available work.
    // This event is called a "dynamic split" and is critical to the dynamic
    // work rebalancing feature. The two obtained sub-tasks are called
    // "parts" of the split.
    // The parts, if concatenated, must represent the same input as would
    // be read by the current task if the split did not happen.
    // The exact way in which the original task is decomposed into the two
    // parts is specified either as a position demarcating them
    // (stop_position), or explicitly as two DerivedSources, if this
    // task consumes a user-defined source type (dynamic_source_split).
    // 
    // The "current" task is adjusted as a result of the split: after a task
    // with range [A, B) sends a stop_position update at C, its range is
    // considered to be [A, C), e.g.:
    // * Progress should be interpreted relative to the new range, e.g.
    //   "75% completed" means "75% of [A, C) completed"
    // * The worker should interpret proposed_stop_position relative to the
    //   new range, e.g. "split at 68%" should be interpreted as
    //   "split at 68% of [A, C)".
    // * If the worker chooses to split again using stop_position, only
    //   stop_positions in [A, C) will be accepted.
    // * Etc.
    // dynamic_source_split has similar semantics: e.g., if a task with
    // source S splits using dynamic_source_split into {P, R}
    // (where P and R must be together equivalent to S), then subsequent
    // progress and proposed_stop_position should be interpreted relative
    // to P, and in a potential subsequent dynamic_source_split into {P', R'},
    // P' and R' must be together equivalent to P, etc.
    Position stopPosition = 12;
    // Total time the worker spent being throttled by external systems.
    double totalThrottlerWaitTimeSeconds = 13;
    // Identifies the WorkItem.
    string workItemId = 14;
}

message WorkerHealthReport {
    // The pods running on the worker. See:
    // http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_pod
    // 
    // This field is used by the worker to send the status of the indvidual
    // containers running on each worker.
    message Pod {
    }
    repeated Pod pods = 1;
    // The interval at which the worker is sending health reports.
    // The default value of 0 should be interpreted as the field is not being
    // explicitly set by the worker.
    string reportInterval = 2;
    // Whether the VM is healthy.
    bool vmIsHealthy = 3;
    // The time the VM was booted.
    string vmStartupTime = 4;
}

message WorkerHealthReportResponse {
    // A positive value indicates the worker should change its reporting interval
    // to the specified value.
    // 
    // The default value of zero means no change in report rate is requested by
    // the server.
    string reportInterval = 1;
}

message WorkerMessage {
    // Labels are used to group WorkerMessages.
    // For example, a worker_message about a particular container
    // might have the labels:
    // { "JOB_ID": "2015-04-22",
    //   "WORKER_ID": "wordcount-vm-2015…"
    //   "CONTAINER_TYPE": "worker",
    //   "CONTAINER_ID": "ac1234def"}
    // Label tags typically correspond to Label enum values. However, for ease
    // of development other strings can be used as tags. LABEL_UNSPECIFIED should
    // not be used here.
    map<string, string> labels = 1;
    // The timestamp of the worker_message.
    string time = 2;
    // The health of a worker.
    WorkerHealthReport workerHealthReport = 3;
    // A worker message code.
    WorkerMessageCode workerMessageCode = 4;
    // Resource metrics reported by workers.
    ResourceUtilizationReport workerMetrics = 5;
    // Shutdown notice by workers.
    WorkerShutdownNotice workerShutdownNotice = 6;
}

message WorkerMessageCode {
    // The code is a string intended for consumption by a machine that identifies
    // the type of message being sent.
    // Examples:
    //  1. "HARNESS_STARTED" might be used to indicate the worker harness has
    //      started.
    //  2. "GCS_DOWNLOAD_ERROR" might be used to indicate an error downloading
    //     a GCS file as part of the boot process of one of the worker containers.
    // 
    // This is a string and not an enum to make it easy to add new codes without
    // waiting for an API change.
    string code = 1;
    // Parameters contains specific information about the code.
    // 
    // This is a struct to allow parameters of different types.
    // 
    // Examples:
    //  1. For a "HARNESS_STARTED" message parameters might provide the name
    //     of the worker and additional data like timing information.
    //  2. For a "GCS_DOWNLOAD_ERROR" parameters might contain fields listing
    //     the GCS objects being downloaded and fields containing errors.
    // 
    // In general complex data structures should be avoided. If a worker
    // needs to send a specific and complicated data structure then please
    // consider defining a new proto and adding it to the data oneof in
    // WorkerMessageResponse.
    // 
    // Conventions:
    //  Parameters should only be used for information that isn't typically passed
    //  as a label.
    //  hostname and other worker identifiers should almost always be passed
    //  as labels since they will be included on most messages.
    map<string, > parameters = 2;
}

message WorkerMessageResponse {
    // The service's response to a worker's health report.
    WorkerHealthReportResponse workerHealthReportResponse = 1;
    // Service's response to reporting worker metrics (currently empty).
    ResourceUtilizationReportResponse workerMetricsResponse = 2;
    // Service's response to shutdown notice (currently empty).
    WorkerShutdownNoticeResponse workerShutdownNoticeResponse = 3;
}

message WorkerPool {
    // Settings for autoscaling of this WorkerPool.
    AutoscalingSettings autoscalingSettings = 1;
    // Data disks that are used by a VM in this workflow.
    repeated Disk dataDisks = 2;
    // The default package set to install.  This allows the service to
    // select a default set of packages which are useful to worker
    // harnesses written in a particular language.
    enum WorkerPool_DefaultPackageSet {
        WORKERPOOL_DEFAULTPACKAGESET_DEFAULT_PACKAGE_SET_UNKNOWN = 0;
        WORKERPOOL_DEFAULTPACKAGESET_DEFAULT_PACKAGE_SET_NONE = 1;
        WORKERPOOL_DEFAULTPACKAGESET_DEFAULT_PACKAGE_SET_JAVA = 2;
        WORKERPOOL_DEFAULTPACKAGESET_DEFAULT_PACKAGE_SET_PYTHON = 3;
    }
    WorkerPool_DefaultPackageSet defaultPackageSet = 3;
    // Size of root disk for VMs, in GB.  If zero or unspecified, the service will
    // attempt to choose a reasonable default.
    int32 diskSizeGb = 4;
    // Fully qualified source image for disks.
    string diskSourceImage = 5;
    // Type of root disk for VMs.  If empty or unspecified, the service will
    // attempt to choose a reasonable default.
    string diskType = 6;
    // Configuration for VM IPs.
    enum WorkerPool_IpConfiguration {
        WORKERPOOL_IPCONFIGURATION_WORKER_IP_UNSPECIFIED = 0;
        WORKERPOOL_IPCONFIGURATION_WORKER_IP_PUBLIC = 1;
        WORKERPOOL_IPCONFIGURATION_WORKER_IP_PRIVATE = 2;
    }
    WorkerPool_IpConfiguration ipConfiguration = 7;
    // The kind of the worker pool; currently only `harness` and `shuffle`
    // are supported.
    string kind = 8;
    // Machine type (e.g. "n1-standard-1").  If empty or unspecified, the
    // service will attempt to choose a reasonable default.
    string machineType = 9;
    // Metadata to set on the Google Compute Engine VMs.
    map<string, string> metadata = 10;
    // Network to which VMs will be assigned.  If empty or unspecified,
    // the service will use the network "default".
    string network = 11;
    // The number of threads per worker harness. If empty or unspecified, the
    // service will choose a number of threads (according to the number of cores
    // on the selected machine type for batch, or 1 by convention for streaming).
    int32 numThreadsPerWorker = 12;
    // Number of Google Compute Engine workers in this pool needed to
    // execute the job.  If zero or unspecified, the service will
    // attempt to choose a reasonable default.
    int32 numWorkers = 13;
    // The action to take on host maintenance, as defined by the Google
    // Compute Engine API.
    string onHostMaintenance = 14;
    // Packages to be installed on workers.
    repeated Package packages = 15;
    // Extra arguments for this worker pool.
    map<string, > poolArgs = 16;
    // Subnetwork to which VMs will be assigned, if desired.  Expected to be of
    // the form "regions/REGION/subnetworks/SUBNETWORK".
    string subnetwork = 17;
    // Settings passed through to Google Compute Engine workers when
    // using the standard Dataflow task runner.  Users should ignore
    // this field.
    TaskRunnerSettings taskrunnerSettings = 18;
    // Sets the policy for determining when to turndown worker pool.
    // Allowed values are: `TEARDOWN_ALWAYS`, `TEARDOWN_ON_SUCCESS`, and
    // `TEARDOWN_NEVER`.
    // `TEARDOWN_ALWAYS` means workers are always torn down regardless of whether
    // the job succeeds. `TEARDOWN_ON_SUCCESS` means workers are torn down
    // if the job succeeds. `TEARDOWN_NEVER` means the workers are never torn
    // down.
    // 
    // If the workers are not torn down by the service, they will
    // continue to run and use Google Compute Engine VM resources in the
    // user's project until they are explicitly terminated by the user.
    // Because of this, Google recommends using the `TEARDOWN_ALWAYS`
    // policy except for small, manually supervised test jobs.
    // 
    // If unknown or unspecified, the service will attempt to choose a reasonable
    // default.
    enum WorkerPool_TeardownPolicy {
        WORKERPOOL_TEARDOWNPOLICY_TEARDOWN_POLICY_UNKNOWN = 0;
        WORKERPOOL_TEARDOWNPOLICY_TEARDOWN_ALWAYS = 1;
        WORKERPOOL_TEARDOWNPOLICY_TEARDOWN_ON_SUCCESS = 2;
        WORKERPOOL_TEARDOWNPOLICY_TEARDOWN_NEVER = 3;
    }
    WorkerPool_TeardownPolicy teardownPolicy = 19;
    // Required. Docker container image that executes the Cloud Dataflow worker
    // harness, residing in Google Container Registry.
    string workerHarnessContainerImage = 20;
    // Zone to run the worker pools in.  If empty or unspecified, the service
    // will attempt to choose a reasonable default.
    string zone = 21;
}

message WorkerSettings {
    // The base URL for accessing Google Cloud APIs.
    // 
    // When workers access Google Cloud APIs, they logically do so via
    // relative URLs.  If this field is specified, it supplies the base
    // URL to use for resolving these relative URLs.  The normative
    // algorithm used is defined by RFC 1808, "Relative Uniform Resource
    // Locators".
    // 
    // If not specified, the default value is "http://www.googleapis.com/"
    string baseUrl = 1;
    // Whether to send work progress updates to the service.
    bool reportingEnabled = 2;
    // The Cloud Dataflow service path relative to the root URL, for example,
    // "dataflow/v1b3/projects".
    string servicePath = 3;
    // The Shuffle service path relative to the root URL, for example,
    // "shuffle/v1beta1".
    string shuffleServicePath = 4;
    // The prefix of the resources the system should use for temporary
    // storage.
    // 
    // The supported resource type is:
    // 
    // Google Cloud Storage:
    // 
    //   storage.googleapis.com/{bucket}/{object}
    //   bucket.storage.googleapis.com/{object}
    string tempStoragePrefix = 5;
    // The ID of the worker running this pipeline.
    string workerId = 6;
}

message WorkerShutdownNotice {
    // The reason for the worker shutdown.
    // Current possible values are:
    //   "UNKNOWN": shutdown reason is unknown.
    //   "PREEMPTION": shutdown reason is preemption.
    // Other possible reasons may be added in the future.
    string reason = 1;
}

message WorkerShutdownNoticeResponse {
}

message WriteInstruction {
    // The input.
    InstructionInput input = 1;
    // The sink to write to.
    Sink sink = 2;
}

enum Alt {
    JSON = 0;
    MEDIA = 1;
    PROTO = 2;
}

service GoogleDataflowService {
    // Send a worker_message to the service.
    rpc PostV1b3ProjectsProjectIdWorkerMessages(PostV1b3ProjectsProjectIdWorkerMessagesRequest) returns (SendWorkerMessagesResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/WorkerMessages"
        body: "body"
      };
    }
    // List the jobs of a project in a given region.
    rpc GetV1b3ProjectsProjectIdJobs(GetV1b3ProjectsProjectIdJobsRequest) returns (ListJobsResponse) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/jobs"
      };
    }
    // Creates a Cloud Dataflow job.
    rpc PostV1b3ProjectsProjectIdJobs(PostV1b3ProjectsProjectIdJobsRequest) returns (Job) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/jobs"
        body: "body"
      };
    }
    // Gets the state of the specified Cloud Dataflow job.
    rpc GetV1b3ProjectsProjectIdJobsJobId(GetV1b3ProjectsProjectIdJobsJobIdRequest) returns (Job) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/jobs/{jobId}"
      };
    }
    // Updates the state of an existing Cloud Dataflow job.
    rpc PutV1b3ProjectsProjectIdJobsJobId(PutV1b3ProjectsProjectIdJobsJobIdRequest) returns (Job) {
      option (google.api.http) = {
        put: "//v1b3/projects/{projectId}/jobs/{jobId}"
        body: "body"
      };
    }
    // Get encoded debug configuration for component. Not cacheable.
    rpc PostV1b3ProjectsProjectIdJobsJobIdDebugGetConfig(PostV1b3ProjectsProjectIdJobsJobIdDebugGetConfigRequest) returns (GetDebugConfigResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/jobs/{jobId}/debug/getConfig"
        body: "body"
      };
    }
    // Send encoded debug capture data for component.
    rpc PostV1b3ProjectsProjectIdJobsJobIdDebugSendCapture(PostV1b3ProjectsProjectIdJobsJobIdDebugSendCaptureRequest) returns (SendDebugCaptureResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/jobs/{jobId}/debug/sendCapture"
        body: "body"
      };
    }
    // Request the job status.
    rpc GetV1b3ProjectsProjectIdJobsJobIdMessages(GetV1b3ProjectsProjectIdJobsJobIdMessagesRequest) returns (ListJobMessagesResponse) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/jobs/{jobId}/messages"
      };
    }
    // Request the job status.
    rpc GetV1b3ProjectsProjectIdJobsJobIdMetrics(GetV1b3ProjectsProjectIdJobsJobIdMetricsRequest) returns (JobMetrics) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/jobs/{jobId}/metrics"
      };
    }
    // Leases a dataflow WorkItem to run.
    rpc PostV1b3ProjectsProjectIdJobsJobIdWorkItems:Lease(PostV1b3ProjectsProjectIdJobsJobIdWorkItems:LeaseRequest) returns (LeaseWorkItemResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/jobs/{jobId}/workItems:lease"
        body: "body"
      };
    }
    // Reports the status of dataflow WorkItems leased by a worker.
    rpc PostV1b3ProjectsProjectIdJobsJobIdWorkItems:ReportStatus(PostV1b3ProjectsProjectIdJobsJobIdWorkItems:ReportStatusRequest) returns (ReportWorkItemStatusResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/jobs/{jobId}/workItems:reportStatus"
        body: "body"
      };
    }
    // List the jobs of a project across all regions.
    rpc GetV1b3ProjectsProjectIdJobs:Aggregated(GetV1b3ProjectsProjectIdJobs:AggregatedRequest) returns (ListJobsResponse) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/jobs:aggregated"
      };
    }
    // Send a worker_message to the service.
    rpc PostV1b3ProjectsProjectIdLocationsLocationWorkerMessages(PostV1b3ProjectsProjectIdLocationsLocationWorkerMessagesRequest) returns (SendWorkerMessagesResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/WorkerMessages"
        body: "body"
      };
    }
    // List the jobs of a project in a given region.
    rpc GetV1b3ProjectsProjectIdLocationsLocationJobs(GetV1b3ProjectsProjectIdLocationsLocationJobsRequest) returns (ListJobsResponse) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/locations/{location}/jobs"
      };
    }
    // Creates a Cloud Dataflow job.
    rpc PostV1b3ProjectsProjectIdLocationsLocationJobs(PostV1b3ProjectsProjectIdLocationsLocationJobsRequest) returns (Job) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/jobs"
        body: "body"
      };
    }
    // Gets the state of the specified Cloud Dataflow job.
    rpc GetV1b3ProjectsProjectIdLocationsLocationJobsJobId(GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdRequest) returns (Job) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}"
      };
    }
    // Updates the state of an existing Cloud Dataflow job.
    rpc PutV1b3ProjectsProjectIdLocationsLocationJobsJobId(PutV1b3ProjectsProjectIdLocationsLocationJobsJobIdRequest) returns (Job) {
      option (google.api.http) = {
        put: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}"
        body: "body"
      };
    }
    // Get encoded debug configuration for component. Not cacheable.
    rpc PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdDebugGetConfig(PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdDebugGetConfigRequest) returns (GetDebugConfigResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}/debug/getConfig"
        body: "body"
      };
    }
    // Send encoded debug capture data for component.
    rpc PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdDebugSendCapture(PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdDebugSendCaptureRequest) returns (SendDebugCaptureResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}/debug/sendCapture"
        body: "body"
      };
    }
    // Request the job status.
    rpc GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMessages(GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMessagesRequest) returns (ListJobMessagesResponse) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}/messages"
      };
    }
    // Request the job status.
    rpc GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMetrics(GetV1b3ProjectsProjectIdLocationsLocationJobsJobIdMetricsRequest) returns (JobMetrics) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}/metrics"
      };
    }
    // Leases a dataflow WorkItem to run.
    rpc PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdWorkItems:Lease(PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdWorkItems:LeaseRequest) returns (LeaseWorkItemResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}/workItems:lease"
        body: "body"
      };
    }
    // Reports the status of dataflow WorkItems leased by a worker.
    rpc PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdWorkItems:ReportStatus(PostV1b3ProjectsProjectIdLocationsLocationJobsJobIdWorkItems:ReportStatusRequest) returns (ReportWorkItemStatusResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/jobs/{jobId}/workItems:reportStatus"
        body: "body"
      };
    }
    // Creates a Cloud Dataflow job from a template.
    rpc PostV1b3ProjectsProjectIdLocationsLocationTemplates(PostV1b3ProjectsProjectIdLocationsLocationTemplatesRequest) returns (Job) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/templates"
        body: "body"
      };
    }
    // Get the template associated with a template.
    rpc GetV1b3ProjectsProjectIdLocationsLocationTemplates:Get(GetV1b3ProjectsProjectIdLocationsLocationTemplates:GetRequest) returns (GetTemplateResponse) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/locations/{location}/templates:get"
      };
    }
    // Launch a template.
    rpc PostV1b3ProjectsProjectIdLocationsLocationTemplates:Launch(PostV1b3ProjectsProjectIdLocationsLocationTemplates:LaunchRequest) returns (LaunchTemplateResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/locations/{location}/templates:launch"
        body: "body"
      };
    }
    // Creates a Cloud Dataflow job from a template.
    rpc PostV1b3ProjectsProjectIdTemplates(PostV1b3ProjectsProjectIdTemplatesRequest) returns (Job) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/templates"
        body: "body"
      };
    }
    // Get the template associated with a template.
    rpc GetV1b3ProjectsProjectIdTemplates:Get(GetV1b3ProjectsProjectIdTemplates:GetRequest) returns (GetTemplateResponse) {
      option (google.api.http) = {
        get: "//v1b3/projects/{projectId}/templates:get"
      };
    }
    // Launch a template.
    rpc PostV1b3ProjectsProjectIdTemplates:Launch(PostV1b3ProjectsProjectIdTemplates:LaunchRequest) returns (LaunchTemplateResponse) {
      option (google.api.http) = {
        post: "//v1b3/projects/{projectId}/templates:launch"
        body: "body"
      };
    }
}
